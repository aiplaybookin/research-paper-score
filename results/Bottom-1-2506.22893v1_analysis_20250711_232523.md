# Research Paper Analysis: Bottom-1-2506.22893v1.pdf

## Summary

| Metric | Value |
|--------|-------|
| **Paper** | Bottom-1-2506.22893v1.pdf |
| **Analysis Date** | 2025-07-11 23:25:23 |
| **Overall Score** | **6.75/10** |
| **Sections Analyzed** | 8 |
| **Processing Time** | 71.25 seconds |
| **Model Used** | claude-3-5-haiku-20241022 |

## Score Distribution

- **Ntroduction**: 7.0/10 üü©üü©üü©üü©üü©üü©üü©‚¨ú‚¨ú‚¨ú
- **Abstracting With Credit Is Permitted. To Copy Otherwise, Or Republish, To Post On Servers Or To Redistribute To Lists, Requires**: 7.0/10 üü©üü©üü©üü©üü©üü©üü©‚¨ú‚¨ú‚¨ú
- **Approach To Tasks. Enterprise Users Remain Accountable For Their Tasks And Sub-Tasks That Contribute**: 8.0/10 üü©üü©üü©üü©üü©üü©üü©üü©‚¨ú‚¨ú
- **Discussions With Other Companies And Tech Developers). No Llm Has Access To This Information**: 4.0/10 üü©üü©üü©üü©‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú
- **Analysis And Insights. These Two Analysts Differ In Their Agencies And User-Centric Ai Ought To**: 7.0/10 üü©üü©üü©üü©üü©üü©üü©‚¨ú‚¨ú‚¨ú
- **Conclusion**: 8.0/10 üü©üü©üü©üü©üü©üü©üü©üü©‚¨ú‚¨ú
- **References**: 8.0/10 üü©üü©üü©üü©üü©üü©üü©üü©‚¨ú‚¨ú
- **Experimental Social Psychology. Vol. 30. Elsevier, 1‚Äì46.**: 5.0/10 üü©üü©üü©üü©üü©‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú

---

## Detailed Section Analysis

### 1. Ntroduction

**Score: 7.0/10**

**Evaluation:**
Score: 7

Reasoning:

Clarity and Writing Quality (1.5/2):
The introduction is generally well-written with clear prose and a coherent flow of ideas. It effectively sets up the context of AI's current state and highlights a critical problem space. The language is professional and articulate, though there's a minor typo in the section header ("Ntroduction" instead of "Introduction").

Technical Depth (1.5/2):
The section demonstrates good technical depth by referencing specific AI technologies (transformers, diffusion models) and touching on complex concepts like AI agents and user-AI interaction paradigms. The authors effectively highlight the current limitations of AI-centric approaches.

Novelty and Originality (1.5/2):
The introduction presents a novel perspective by critiquing the current "AI-Centric User" model and emphasizing the need for more user-focused AI development. This framing offers an interesting and original angle on AI interaction challenges.

Methodology Rigor (1/2):
While not a methodology section, the introduction sets up a clear problem statement. However, it could benefit from a more explicit outline of the research approach or proposed solution.

Evidence and Support (1.5/2):
The authors support their claims with references to academic literature (citations like [36], [16], [7]) and a McKinsey report, demonstrating a good balance of academic and industry-based evidence.

Strengths:
- Crisp articulation of AI's current state
- Clear problem identification
- Good use of references
- Engaging writing style

Areas for Improvement:
- Slightly more explicit research question/approach
- Fix the typo in the section header
- Potentially elaborate on the proposed solution's overview

The introduction effectively sets the stage for the research, providing context, identifying a critical problem, and generating reader interest.

<details>
<summary>üìÑ View Section Content (1881 characters)</summary>

```
Introduction
The promise of Artificial Intelligence (AI) has been prognosticated for decades. The pace has
decidedly picked up over the last three years with apparent versatility of transformers [36] and
diffusion models [16] for text and image based data, respectively. There has been a proliferation of
AI Assistants‚Äìsuch as chatbots and co-pilots‚Äìin personal and professional domains, albeit yielding
mixed results for enterprises1. At the same time, discussions about Agents [7, 17, 31] have provided
a fillip for AI. However, lost in all this is the headwind coming from the current, widely practiced
regime of AI-Centric User, whereby humans do the heavy lift of adjusting to inflexible AI. For
example, (i) a typical user must learn to post queries in a specific manner to get appropriate and
useful responses from AI [9]; or, (ii) AI models, aligned with general purpose user preferences,
miss out on the needs for specific purpose, or for processes and workflows, or of specialists and
experienced users [33]. Moreover, most of the development focus of enterprises is on the AI itself,
less so on the expectations of users.
1https://www.mckinsey.com/capabilities/quantumblack/our-insights/seizing-the-agentic-ai-advantage
Authors‚Äô Contact Information: Arpit Narechania, arpit@ust.hk, The Hong Kong University of Science and Technology,
Hong Kong SAR, China; Alex Endert, Georgia Institute of Technology, Atlanta, USA, endert@gatech.edu; Atanu R Sinha,
Adobe Research, Bengaluru, India, atr@adobe.com.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the
full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.
```

</details>

---

### 2. Abstracting With Credit Is Permitted. To Copy Otherwise, Or Republish, To Post On Servers Or To Redistribute To Lists, Requires

**Score: 7.0/10**

**Evaluation:**
Score: 7/10

Reasoning:

Clarity and Writing Quality (1.5/2):
- The text is relatively clear and well-structured
- Some technical terminology is well-explained
- Sentences are somewhat complex but mostly comprehensible

Technical Depth (1.5/2):
- Introduces nuanced concepts like AI-Centric vs User-Centric AI
- Defines key terms like "Agents" with academic precision
- Demonstrates understanding of AI deployment paradigms

Novelty and Originality (1/2):
- Proposes an interesting shift in AI paradigm from AI-Centric to User-Centric
- Suggests a novel perspective on enterprise AI deployment
- Could benefit from more explicit novel contributions

Methodology Rigor (1/2):
- Limited methodology details in this excerpt
- Provides conceptual framework but lacks explicit methodological explanation
- More methodological details would strengthen the section

Evidence and Support (1/2):
- References some external sources [3, 10, 27]
- Lacks detailed empirical evidence in this section
- Makes assertive claims that would benefit from additional substantiation

The section shows promise in reframing AI deployment perspectives, particularly in enterprise contexts. It introduces compelling concepts but would benefit from more concrete methodological and empirical support. The writing is professional and academically oriented, demonstrating potential for a substantive research contribution.

<details>
<summary>üìÑ View Section Content (2029 characters)</summary>

```
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
¬© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM 1557-735X/2018/8-ART111
https://doi.org/XXXXXXX.XXXXXXX
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
111:2
Narechania, Endert, and Sinha
To wit, we refer to AI as the capability of a tool and the tool itself, including but not limited to
generative AI (GenAI). We posit that to realize the promise of AI, a shift away from the paradigm
of AI-Centric User to models of User-Centric AI is needed, wherein such AI adapt according
to users, their specific tasks, workflows, and processes. For enterprises, we assert that the shift
is attainable through task-specific Agents and their efficient and effective organization. We refer
to Agents as systems that independently accomplish tasks on a user‚Äôs behalf [27]. Hitherto, the
operational lens of industry views Agents through alternative agentic frameworks that pivot around
the paradigm of AI-Centric User [3, 10, 27]. In this paper, we take a step back and view the space
of AI and its delivery via Agents through the lens of users and beneficiaries (hereafter, users) with
focus on Enterprise Decision-Making, an important testbed and a milestone for progress in AI
and agentic thinking. We offer technological success criteria for efficient and effective impact of AI
on enterprises, toward a vision of User-Centric AI.
Embracing User-Centric AI in Enterprise Decision-Making
Our labeling of current efforts around GenAI as AI-Centric User stems from the heavy lift
enterprise users do trying to extract ever more from GenAI, as if, it is the ultimate repository,
whose yield is limited only by the incapability of users. Our proposed User-Centric AI shifts
the burden to AI itself, which must incorporate user‚Äôs agency, their expectations, and also their
```

</details>

---

### 3. Approach To Tasks. Enterprise Users Remain Accountable For Their Tasks And Sub-Tasks That Contribute

**Score: 8.0/10**

**Evaluation:**
Score: 8

Reasoning:

1. Clarity and Writing Quality (2/2): 
- The text is exceptionally well-written with clear, articulate language
- Complex technical concepts are explained systematically and comprehensively
- Logical flow and structure make the content easy to follow

2. Technical Depth (2/2):
- Demonstrates profound technical understanding of AI, enterprise decision-making, and user-centric frameworks
- Provides nuanced analysis of AI challenges in enterprise contexts
- Introduces sophisticated conceptual tenets and primitives with rigorous explanations

3. Novelty and Originality (1.5/2):
- Presents innovative approach to user-centric AI in enterprise settings
- Introduces unique tenets like "Locally Privacy Preserving Agent-Learning" and "Market Mechanism Platform"
- Offers fresh perspective on AI agent design and deployment

4. Methodology Rigor (1.5/2):
- Establishes clear methodological assumptions and boundary conditions
- Provides detailed rationales for each proposed tenet
- Uses concrete workflow examples to illustrate theoretical concepts

5. Evidence and Support (1/2):
- References multiple external sources and research
- Includes practical examples and hypothetical scenarios
- Could benefit from more empirical evidence or case studies to substantiate claims

Strengths include the comprehensive framework, deep technical insights, and systematic approach to reimagining AI in enterprise contexts. Minor areas for improvement relate to providing more direct empirical validation.

<details>
<summary>üìÑ View Section Content (27801 characters)</summary>

```
approach to tasks. Enterprise users remain accountable for their tasks and sub-tasks that contribute
to decision-making, whether strategic or tactical. In addition, these users‚Äô tasks and decisions often
have different dependencies with and accountability to other users. Subsuming these user-centric
concepts in AI is an important step to attain its potential for Enterprise Decision-Making.
Enterprises, their Users, and Decision-Making. The long term viability of AI and its delivery
via Agents rides on capital expenditure by enterprises23, yet ‚Äúonly 1 percent of company executives
describe their GenAI rollouts as ‚Äúmature‚Äù4.‚Äù As well, ‚Äúdespite the endless announcements about how
firms are ushering AI into their operations, few make much use of the technology for serious work5.‚Äù
The success of AI depends on the nature of expectations and net benefits that may accrue from
enterprises‚Äô AI usage, which notably is qualitatively different from AI usage in the personal domain.
Enterprise users are accountable to someone else, making their behaviors different from their
individual self in personal situations. For example, many users may participate in a task; a user‚Äôs
task and solution may be customized; value of a tool is determined by repeated usage; the demand
for a tool has enterprise-specific dependencies on tasks and tools that others use; and so on6. All this
makes development of AI for enterprises challenging; but with a large upside if the challenges can
be overcome7. Agents can help, only if AI overcomes the lack of a deep understanding of enterprise
users. For enterprises, we particularly drill down on decision-making due to its omnipresent role, its
ambiguity and uncertainty, its time-consuming nature, and wider opportunity for impact-of-AI, by
moving from human decision-making to more automated decision-making. For AI, understanding
enterprise decision-making allows a discrete change from low-value-added AI Assistants to high-
value added Agents. We identify six tenets that can be infused in User-Centric AI, and contribute
2https://a16z.com/ai-enterprise-2025/
3https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/the-cost-of-compute-
a-7-trillion-dollar-race-to-scale-data-centers
4https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai
5https://www.economist.com/finance-and-economics/2025/05/26/why-ai-hasnt-taken-your-job
6https://hbsp.harvard.edu/product/8145-PDF-ENG
7https://www.mckinsey.com/capabilities/quantumblack/our-insights/seizing-the-agentic-ai-advantage
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
Agentic Enterprise
111:3
substantively toward AI and organization of Agents into Platforms. Our vision complements
existing principles, practices, and lessons for successful adoption of AI in enterprises [3, 26, 29].
Notably, prior work on human-centered AI [34] and human-AI collaboration [30, 37], predating
large language models (LLMs) and GenAI, emphasize human values, ethics, and usability. In this
paper, we contribute by carving out such conceptual gaps in current AI research. We particularly
draw attention to the underexplored, albeit central role of users, their agency, and their contribution
to the ecosystems. We raise questions around the hidden labor of users who not only utilize AI, but
also contribute to it either directly during training or by providing explicit feedback to AI responses,
or indirectly through providing their data passively while using a system. Unlike general-purpose AI
frameworks, we advocate deep integration of AI with users‚Äô beliefs, expectations, tasks, workflows
and decision-making contexts. Additionally, in a user‚Äôs Workflow involving AI, we try to foresee
points of commonality and conflict between Users and Agents, and propose coordination-aware,
market-mechanism-oriented Platforms of specialized AI agents to foster commonality and diffuse
conflict. This shift represents a major rethinking of Agents as incentive-compatible and responsive
partners which can even improve the user cognitively. Lastly, with traditional models proven in
many analytic tasks for enterprises, it is necessary to offer flexibility in AI to complement those.
Thus, the unique challenges and opportunities of AI in a user-centric paradigm may require thinking
beyond GenAI and Large Language Models (LLMs).
Six Tenets of User-Centric AI in Enterprises
1. Primacy of Process-Orientation in AI.
2. Forward Thinking AI.
3. Locally Privacy Preserving Agent-Learning.
4. Market Mechanism Platform for Agents and Users.
5. Agent Risk-Reward Diversity and Quality-Price Diversity.
6. Low Entry, Exit Barriers for Agents.
2
The Premise
As part of our premise, we consider three entities‚ÄìUsers, Agents, Platforms‚Äìand a Workflow.
Below, we define and clearly delineate a set of assumptions to be used as building blocks for the
rest of the paper. These assumptions work as boundary conditions for our propositions.
Users: A group which may benefit from AI. They could be performing tasks for an enterprise
collectively, or, individually. We make the following assumptions about users:
UA1 User‚Äôs Agency: Users seek agency and vary in skills from other users, and across tasks.
UA2 Discoverability: Users self-select whether to discover AI tools and not the other way round.
UA3 User Heterogeneity: For a given task, users are diverse in expectations and utilities from AI.
UA4 Task Heterogeneity: Across tasks, a user has diverse expectations and utilities from AI.
UA5 Utility: Users aim to maximize their own utility from AI.
UA6 Privacy: Users vary in expectations about degrees of protection of information they share.
Agents: ‚ÄúAgents are systems that independently accomplish tasks on your behalf‚Äù [27] by using a
language model to manage workflows, make decisions, correct errors, and dynamically interact
with external tools. Essentially, they are entities through which AI is delivered to users. These could
be a single entity or a collection of entities. We make the following assumptions about agents:
AA1 Agent Manifestation: An agent can be a user-assisting tool or an autonomous operator.
AA2 Task Specialization: An agent is specialized for a task.
AA3 Degree of Autonomy: An agent can be controlled or autonomous.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
111:4
Narechania, Endert, and Sinha
AA4 Communication Within: Autonomous agents may communicate directly with other agents.
AA5 Communication Outside: Autonomous agents may communicate directly with users.
AA6 Rewards: Agents are endowed with varying rewards by the platform.
AA7 Types of Models: Agents can represent GenAI or traditional models (we are agnostic to the
type) and depend on user demand and supply from the agent organization and platform.
AA8 Disclosure: Agents communicate their capabilities publicly, whether truthful or not.
Platform: A planner, which is an entity in charge of delivering benefits of AI to users. It can be
third-party or enterprise-governed. We make the following assumptions about the planner:
PA1 Accountability: The planner is accountable to users for delivering AI.
PA2 Organization: The planner uses a set of agents.
PA3 Discoverability: Planner needs to discover capabilities of agents.
PA4 Management: The planner manages the agents to maximize own utility.
PA5 Communication: Users communicate with the planner, which communicates with agents.
Planner can use a confederate, or orchestrator, which is then the conduit for communication.
PA6 Safety: The planner is responsible for degrees of safety in outputs. The degree depends on
the task and user‚Äôs stated appetite for risk.
Workflow: ‚ÄúA workflow is a sequence of steps that must be executed to meet the user‚Äôs goal‚Äù [27].
For expositional ease, consider users who perform data analytics as part of their work for an
enterprise‚Äìsmall, medium or large. We define the following simplified workflow‚Äìcomprising four
discrete tasks common in an enterprise setting‚Äìas a running example for the rest of the paper. This
workflow is not meant to be exhaustive, but only an illustration.
1. Data (Preparation)
‚Üí2. Model (Selection, Application)
‚Üí3. Results (Evaluation, Verification)
‚Üí4. Presentation (Inference, Recommendation)
Each of the four tasks has several sub-tasks and lower-level analytic decisions contained within
it. For example, the model selection step consists of gathering a collection of models, ranking or
filtering them based on task criteria, and deciding on either a single model or an ensemble approach.
Having such discrete tasks makes it easier to assign an agent to it. Agents can also collaborate with
each other on a task. Agents can also plan [18, 38] and reason [5, 8] within this workflow to the
extent desired. Lastly, we let users freely choose agents to fit their workflow. For instance, one user
might only need help with model selection, while another might run a model, evaluate it, and then
return to data preparation to create more features.
3
Tenets of User-Centric AI
Under the premise in Section 2, we propose six tenets to guide AI toward an agentic enterprise.
First, we identify three necessary primitives for AI to recognize and endow agents with. Then, we
present the tenets as success factors for agentic platforms serving enterprise users.
3.1
Primitives
We first present three user-first primitives, emphasizing user-centrality.
I. User Agency.
II. Agent Foresight.
III. User Feedback and Agent Learning.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
Agentic Enterprise
111:5
In touting (I), drawing from formidable research in psychology [4], we recognize the importance
of incorporating the following user level criteria: (a) Users‚Äô expectations and beliefs, since these
impact the realization of their experiences of interactions with AI [22, 24]; (b) Users‚Äô tasks often
involve subjectivity, with no single correct answer; only the user fully understands their task and the
potential benefits from AI, making their ex post evaluation as important as any ex ante assessment
of AI; (c) Users are heterogeneous in tasks performed suggesting differences in expectations and
beliefs across tasks; (d) Users are heterogeneous in skill levels for a given task, suggesting differences
in expectations and beliefs for the task as well as benefits sought from the task; (e) Users want
to experiment with AI interactions for their own workflow [13], which only they know by (b),
suggesting expectation of flexibility and experimentation in tools.
In advocating (II), and to incorporate (a)‚Äì(d) above, AI needs to be imbued with Foresight, which
we define as ‚ÄúLooking ahead beyond the immediate next response, suggesting a degree of anticipation
of user‚Äôs reaction to its own response and interjecting for clarifications judiciously and with alternatives.‚Äù
Note that we do not reproduce other characteristics required of AI which are espoused in the extant
literature [1, 19]. We only highlight Foresight in addition to those.
In emphasizing (III), and to complement (I) and (III), incorporation of heterogeneous feedback for
a task and for heterogeneous tasks is necessary for AI to learn to meet and adapt to heterogeneous
users‚Äô expectations and beliefs. Without an infrastructure for multi-user, multi-task, multi-agent
continuous feedback and learning, along with recognition of heterogeneities in users and tasks, no
framework can approach a degree of completeness. For example, incompleteness in user‚Äôs prompts
about tasks can be anticipated to build into the agents‚Äô behaviors. These are especially pertinent
with the talk about AI having turned toward agents and agentic frameworks.
3.2
Tenets
Based on the primitives, we present six tenets as guidelines for enterprises and platforms, who serve
the former. Some apply broadly to enterprise users, others describe desired agent traits, and some
target platforms. Not all tenets are needed in every case; a subset may be enough. Each tenet is
stated with its rationale and illustrated by an example from the Workflow (introduced in Section 2).
3.2.1
Tenet 1: Primacy of Process-Orientation in AI.
Rationale: Broadly, value to users is of two types: (i) Outcome, and (ii) Process. Given UA1, UA3, and
UA4, the sample space of outcomes is not known, and thus, AI cannot meet all users‚Äô expectations
on outcome-quality. Instead, steps or tasks of processes can be offered with respect to workflows,
where different users call in at different tasks in the workflow as and when needed (UA4, AA7, PA2).
Giving users easy-to-assemble tools to build their own workflows is beneficial. This requires AI to
have a deeper understanding of users‚Äô workflows and learn how to improve their efficiency. In other
words, the focus shifts from training models on outcome data to training on process data. This
process-oriented approach shifts responsibility for outcome quality from the platform to the users,
and when focused on process, users retain agency [14, 20, 30] (UA1, UA5). For instance, users can
satisfy their agency through experimentation, trial and error, and achieve more self-actualization.
The platform reduces effort on excessive focus on outcome-quality, which is difficult to satisfy
under the various heterogeneities (UA3, UA4, PA6).
Example: An outcome orientation focuses on task specific response regardless of the user‚Äôs overall
need. An experienced enterprise user may need help with model selection and running, but does
not need help with other tasks. This user knows that model selection depends on both the task prior
to it, data preparation step, and the tasks the user wants after it; say, what type of presentation the
user wants. An outcome orientation is inadequate since it does not consider the user‚Äôs presentation
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
111:6
Narechania, Endert, and Sinha
need. A process orientation by AI allows users to plug in a task within their workflow, which knows
about the presentation need and can inform them about model selection.
3.2.2
Tenet 2: Forward Thinking AI.
Rationale: Enterprise users have diverse expectations and goals across tasks, many involving long-
term planning and complex workflows (UA3‚Äì5, AA2, PA2). Reactive AI platforms that respond
only to current user inputs lack the foresight needed for meaningful strategic support. This limits
AI‚Äôs ability to help users anticipate risks, plan ahead, or manage multi-step processes, leading to
fragmented and short-sighted assistance. Shifting to Forward Thinking AI that proactively forecasts
needs, warns of potential errors early, and guides users forward empowers better decisions and
broader goals (UA5, AA4‚Äì5, PA3).
Example: Forward Thinking AI within a workflow can occur for a task and for the workflow. In
response to a user‚Äôs ask about model selection, the AI can give the best response. A forward-looking
AI can determine additional follow-up questions from the user without her asking and frame the
response accordingly. It can offer the user alternative models with pros and cons rationale with
respect to her expected follow-up questions, which informs user‚Äôs selection. Furthermore, knowing
the user‚Äôs workflow a Forward Thinking AI can consider her presentation need as well.
3.2.3
Tenet 3: Locally Privacy Preserving Agent-Learning.
Rationale: AI platforms that treat user data uniformly and rely on one-size-fits-all agent learning fail
to meet the diverse skills and varied expectations of users across tasks (UA1, UA3‚Äì5, AA6‚Äì7, PA1).
While generalist agents work well for novices, experts benefit from integrating their specialized
knowledge to improve engagement and drive continuous agent improvement. Differentiating data
and feedback by user expertise and privacy preferences enables personalization and respects user
agency (UA1, UA5‚Äì6, AA7), encouraging experts to share valuable knowledge without risking
privacy or competitive concerns (AA7‚ÄìAA8). To meet enterprise needs, platforms (PA1, PA3, PA6)
can use partitioned or federated learning that tags data by expertise and privacy, supports user-
specific agent training, lets users control AI learning contributions, and applies privacy safeguards
with incentives for expert participation. Without expert input, platform appeal drops. This privacy-
focused, on-demand approach builds reputation, improves personalization, and sustains a dynamic
ecosystem where novices and experts drive ongoing innovation and growth.
Example: Consider the data preparation step of our running example Workflow. While prior art
exists for feature selection from data, creating the feature that is most useful for a modeling task is as
much an art and depends on user experience and judgment. As any expert analyst knows, a feature
is important not only based on its contribution to model accuracy, but also on its actionability and
managerial meaningfulness. Those two latter aspects are private information to an expert who may
not want to share even with all users in the same enterprise in order to preserve their own worth.
If the AI Agent gleans and learns this expertise from the user, it can percolate to others, going
against the wishes of the user. Anticipating this, a user may not want to interact with the Agent AI,
reducing its learnability. Giving the user control of what enters into the AI‚Äôs learning maintains
locally privacy preserving agent learning. In other words, an AI can ‚Äòthink global, act local.‚Äô
3.2.4
Tenet 4: Market Mechanism Platform for Agents and Users.
Rationale: Given users‚Äô diverse preferences and risk tolerances (UA1‚Äì5), and the uncertainty in AI
and agent quality relative to user needs (AA2, AA6, PA4), we propose a market mechanism [2, 23].
It features specialized agents offering varied skills, behaviors, and risk-reward profiles, enabling
users to choose agents that best fit their needs, promoting ownership and flexibility in AI outputs
(UA1, UA3, AA2, AA6, PA2, PA4). From the platform perspective, a decentralized agent organization
reduces the risk of reputation damage from any single poor-performing agent, boosting resilience
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
Agentic Enterprise
111:7
and encouraging innovation through competition and collaboration (AA6, PA4‚Äì5). While centralized
control eases management and ensures consistency, it limits user agency and adaptability (UA1, PA4).
Organizing agents as interoperable, loosely coupled entities in a market balances user autonomy
with quality and reputation safeguards, fostering experimentation, customization, and strong
process integration (UA1, UA5, AA2, PA2, PA4‚Äì5). Decentralized does not mean there are no checks
and balances on ‚Äúunderperforming‚Äù agents; it only means the planner‚Äôs role shifts to managing
agents through incentives to ensure user success rather than direct control (PA4, AA6).
Example: Each task-agent discloses its capabilities publicly so that users know what each can and
cannot do; each agent also receives feedback from users after task completion. All this renders
incentive compatible reward to the agent creator / developer to improve the agent. Without a
market mechanism such disclosure and feedback are rendered less effective.
3.2.5
Tenet 5: Agent Risk-Reward Diversity and Quality-Price Diversity.
Rationale: Diversity in agent offerings is necessary to address the heterogeneity of user tasks (UA4,
AA2, AA6). Given the heterogeneity in user expectations and utilities across tasks and the principle
that users aim to maximize their own utility (UA5), AI platforms should allow agents to self-select
their behavior styles and risk-reward profiles (UA1, AA2, AA6, PA6). Since agents cannot fully
eliminate undesirable behaviors due to task heterogeneity, supporting diverse agent behaviors
enables users to match risk profiles to their preferences (UA5, AA5‚Äì6, PA6). Users comfortable with
high-risk, high-reward outcomes can choose more exploratory agents, while others may prefer
safer, conservative ones. This flexibility preserves user agency and fosters innovation, while the
platform manages a diverse set of specialized agents to meet varying user needs, creating a dynamic
ecosystem that balances creativity and safety (UA1, UA5, AA2, PA2, PA6).
Example: Consider the model selection task in our example Workflow. A user with a penchant for
the latest may seek the newest model to showcase to the enterprise, only to be shot down in a
presentation by a senior since the latest model makes evaluation against historical benchmarks
difficult. In other cases, the risk may be worthwhile if the senior finds something useful for the
future from such a model. Another user may want to stay with tried and proven models. For a
single agent, serving both such types of user can be a challenge. Two agents with diverse skills
can address needs of two different users. These differences among agents can reflect in different
qualities as perceived by users and thus different willingness to pay for agent.
3.2.6
Tenet 6: Low Entry, Exit Barriers for Agents.
Rationale: Lower barriers to entry foster development of agents (AA6, AA8, PA5). Creating ease of
exit for poor performers maintains performance excellence that increases utility for users (UA5,
AA6, PA4‚Äì5). For entry, planner‚Äôs gatekeeping role is important, since unlike exit of existing agents,
less objective information about agent is expected (PA2, PA5). Moreover, Tenet 6 catalyzes Tenet 5
(AA8, PA5). We refrain from calling out ‚Äúfree or low‚Äù entry / exit since considerations of sanctity of
each enterprise‚Äôs data / information, that of switching cost for its users, and proclivity of agents
(developers) to overclaim their capability, along with planner‚Äôs imperfect verifiability of claims, all
pose restraints (UA6, PA1, PA6).
Example: A market mechanism works when an agent with low reward for, say model selection,
across a lot of users, falls below a certain threshold and is exited. This also suggests that other
qualified agents are ready to take its place, which lower barrier to entry ensures.
4
Toward User-Centric AI for Agentic Enterprise
Focusing on enterprise decision-making, we propose the type of AI evolution required to pay
persistent dividend for companies that use AI. Whether AI is up to the task, or, what form it will
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
111:8
Narechania, Endert, and Sinha
LEVEL OF DECISION-MAKING
LEVEL OF AUTOMATION
AGENTIC PARADIGM
Tactical
Strategic
AI-Centric User
User-Centric AI
Automated
User
AGENTIC
ENTERPRISE DECISION-MAKING
Future Opportunities
Underperforming Efforts
Successful Efforts
Fig. 1. Agentic Enterprise Decision-Making:
I cautions about intent since it remains difficult to ascertain
merely from data without user intervention.
II highlights the successes of automation for enterprise decision-
making in trading in stock market trading; revenue management for dynamic demand systems such as airlines
and hotels; recommendation systems for online businesses; and ad-bidding online market mechanism‚Äîall
with a combination of traditional, ML and AI models. III exemplifies gaps in common, big enterprise strategic
decisions, none of which is effectively addressable today with any AI. These decisions require enterprise
users‚Äô breaking them down into atomic decisions issues. IV shows only a single atomic decision issue for
each of the three strategic decisions, as broken down by users.
V shows major information gaps, which are
germane for strategic decisions. Next, SOTA models use LLMs in attempts to address atomic issues but fall
short. To address atomic issues, VI shows the use of Decomposition, Planning, Reasoning (DPR), de rigueur
in strategic decision-making by users. VII shows that LLMs attempt to mimic DPR; but is emblematic of
AI-Centric User who tries everything to get the LLMs to perform better; yet not rising to a reliable level.
Finally, VIII highlights the conceptual gaps that are necessary for AI to fill for enterprise decision-making.
take is up for a debate8. For the rest of the paper, we refer to Figure 1. Broadly, decision-making
can be classified into Tactical or Strategic; and Automated or Human, where ‚Äúautomated‚Äù includes
human-in-the-loop automated decisions. Our premise is that enterprises‚Äô preference is to push
more decision-making toward automation (including user-in-the-loop), subject to the decisions
8https://hbr.org/2024/12/the-irreplaceable-value-of-human-decision-making-in-the-age-of-ai
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
Agentic Enterprise
111:9
outperforming those by users. Favoring their preference is the strong evidence of success in tactical
decision-making such as Trading, Revenue Management (e.g., in hotels), Recommendation Systems
(RecSYS), Ad-Bidding, etc., where data based automation is de rigueur
II . Tactical decision-making
requiring knowledge of user intent
I , a difficult construct to infer from data alone or due to lack
of data [32], still benefits from human involvement. For example, solely based on a sequence of
clicks, it is difficult to tell if a new user is only looking for information or wants to make a purchase.
On the strategic decision side, we illustrate with three common examples of enterprise decision
problems. For each, the big question is shown in
III , while a corresponding atomic question is
shown in IV (only one atomic question among several is shown for each big question). The questions
in IV can be answered, in part, as follows by LLMs:
A. Regarding determining the maturity of a technology, an LLM can respond using its global
knowledge gathered from reports and commentaries (about the technology) on the web.
B. Regarding predicting next month‚Äôs buyers, an LLM‚Äôs generic global knowledge cannot answer
it. In a traditional setting, a business may follow a Workflow (akin to the one in Section 2)
to determine the answer. In an LLM setting, a business can provide data about the customer
segments it targets and their past purchase behaviors, along with how those segments are
defined. The LLM can then suggest a segment definition, though it may not be perfect.
C. Regarding determining unprofitable items, a business may follow a traditional Workflow like
B ‚Äôs above. In an LLM setting, the business can feed historical data tables about the different
products, unit sales, prices, and costs, for the LLM to provide an answer quickly. However, even
through a combination of decomposition, planning, and reasoning operations such as chain of
thought and tree of thought [21, 25, 28, 39, 40], LLMs can only partially address these questions.
These responses obtainable from GenAI are evidently restrictive and consistent with the paradigm
of AI-Centric User; thus, VII shows GenAI as a cautionary tale.
On the other hand, the user dependent big questions are more involved for an LLM to do
justice. For instance, in
III A, only the business knows its strength and weaknesses in developing
the technology and converting it into products. For example, does it have quality human capital
to leapfrog the tech beyond what is available or licensed? Does it have the compute and data
resources to develop it internally? How long will internal development take? What will it cost to
make versus buy, in the short and long term? This information depends on goals
V (defensive, to
retain current customers, or offensive, to attract new ones); judgment and subjectivity (different
executives may have different answers, as these are not objectively measurable); environment
(internal technologies and how they fit with the new technology, plus the company‚Äôs view of its
competition); and private knowledge (internal research and confidential insights from C-suite
```

</details>

---

### 4. Discussions With Other Companies And Tech Developers). No Llm Has Access To This Information

**Score: 4.0/10**

**Evaluation:**
Score: 4/10

Reasoning:

1. Clarity and Writing Quality (1/2):
- The text is somewhat disjointed and lacks clear structure
- There are formatting issues and unexplained citations (e.g., [21, 25, 28, 39, 40])
- Some sentences are complex and difficult to parse
- Inconsistent formatting and roman numerals interrupt readability

2. Technical Depth (1/2):
- Touches on interesting concepts like DPR (decomposition, planning, reasoning)
- Discusses enterprise AI challenges
- Lacks substantive technical explanation of the proposed solutions
- Mentions user-centric AI but doesn't provide deep technical insights

3. Novelty and Originality (1/2):
- Highlights some interesting perspectives on enterprise AI limitations
- Discusses user motivation types (prevention vs. promotion)
- Lacks truly novel or groundbreaking insights
- Ideas seem somewhat generic

4. Methodology Rigor (0/2):
- No clear methodological approach
- No experimental design or rigorous methodology presented
- Claims are not substantiated with empirical evidence

5. Evidence and Support (1/2):
- Limited citations
- No concrete evidence or data supporting claims
- Assertions about enterprise AI challenges are not well-supported

The section needs significant improvements in clarity, technical depth, and empirical support. While it touches on interesting concepts, the execution is weak and the arguments are not well-developed.

<details>
<summary>üìÑ View Section Content (1973 characters)</summary>

```
discussions with other companies and tech developers). No LLM has access to this information
V .
Moreover, a business may be reluctant to share confidential data with an LLM without an iron-clad
leakage prevention or developing its own local GenAI model.
Recent advances in decomposition, planning and reasoning (or, DPR) [21, 25, 28, 39, 40] foster
hopes of improved automation, with user-in-the-loop. Currently, utility of DPR to enterprises is
limited VII due to a host of factors including misalignment of goals between AI or Platform and
enterprise users, lack of training of finetuning data that capture the complex and involved nature
of enterprise decisions, incomplete and imperfect input from enterprise users, and dearth of models
that incorporate these gaps. The severe gaps
V with respect to strategic decisions include lack
of information about decision makers‚Äô goals, lack of data (codified information) about human-
judgment, subjectivity (which could also be situational), private knowledge, and environment
(within firm, within industry, etc.). To deal with this ambiguous situation, technology turns to
LLMs to fill in the gaps. Users exert effort to extract as much as they can from LLMs, as if the yield
from LLMs is stymied only by users‚Äô inability.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
111:10
Narechania, Endert, and Sinha
Enterprises can benefit from Agents that are endowed with User-Centric AI VIII . This kind of AI
appreciates human agency, namely, that enterprise users prefer to learn how to do their job, instead
of being told an answer for a specific question. Users motivation plays a role where motivation is one
of two types: prevention or promotion [15], and they are situational and contextual. As illustration,
for an analyst with prevention focus, not making an error in presenting results to a large team is
more important, while one with a promotion focus may want to surprise the team with a very new
```

</details>

---

### 5. Analysis And Insights. These Two Analysts Differ In Their Agencies And User-Centric Ai Ought To

**Score: 7.0/10**

**Evaluation:**
Score: 7/10

Reasoning:

Clarity and Writing Quality (1.5/2):
- The text is generally clear and articulates complex ideas about AI systems
- Some sentences are dense and could benefit from simplification
- Technical terminology is used appropriately

Technical Depth (1.5/2):
- Demonstrates substantive understanding of enterprise AI dynamics
- Discusses nuanced concepts like agency interactions, cooperation/competition contexts
- Explores practical considerations for AI platform design

Novelty and Originality (1.5/2):
- Proposes an interesting "Walled-Garden Platform" concept
- Integrates insights from economics and organizational behavior
- Offers novel perspectives on AI agent governance

Methodology Rigor (1/2):
- Conceptual proposal lacks detailed implementation specifics
- Cites references but doesn't deeply explain research methodology
- Theoretical framework is sound but needs more empirical grounding

Evidence and Support (1.5/2):
- Multiple academic citations demonstrate scholarly approach
- References economic and management research literature
- Provides conceptual arguments supported by references

Strengths:
- Thoughtful exploration of enterprise AI challenges
- Balanced approach considering user, agent, and organizational perspectives

Areas for Improvement:
- More concrete implementation details
- Clearer empirical validation of proposed platform
- Slightly complex language could be simplified

The score reflects a strong conceptual contribution with room for more rigorous methodological development.

<details>
<summary>üìÑ View Section Content (2380 characters)</summary>

```
analysis and insights. These two analysts differ in their agencies and User-Centric AI ought to
take the difference into account. It is common in enterprises that users cooperate on some work
and compete on others [12, 35]. This has ramifications for Agentic AI, with which all users interact,
in that the AI should foster improved cooperation with smoother flow of information among users,
but for the latter curtail information flow between users perhaps with sandboxes. Moreover, a firm
cooperates with some partner-firms and competes with rival-firms. This understanding, some of
which can come from firm-specific information, is crucial for AI to offer decision-making advice
and input or make automated decisions on many aspects. Last but not the least, the omnipresence
of cognitive biases in managerial judgment and decision-making is well received wisdom [6]. In
offering advice to users for decision-making, where users provide input to the AI, detecting such
biases in the input affords AI to perform mitigation in advice giving.
What kind of Agentic Platform to benefit Enterprises? We propose a Walled-Garden Platform
with agent autonomy, governed by a market mechanism. Enterprises have data and knowledge
repositories that need firewall. Without a degree of control exerted over agents, enterprises may find
inadequate protection for all their repositories and reduce adoption. At the same time, a centralized
planner through which users must access services of agents can be costly in communication (say,
planner seeking too many clarifications), poor user experience in not being able to communicate
with an agent she specifically needs help from, wrong interpretation of user intent, direction
of query to wrong agent, mis-attribution of user‚Äôs feedback to agents, among others. A walled
garden platform with direct communication between users and agents, where users beware of
their own actions, planners supervise agents, and agents are rewarded based on‚Äìtheir own claim
of performance, planner‚Äôs observance of their performance and users‚Äô feedback‚Äìis worthy of
strong consideration. Borrowing from the large market mechanism literature in Economics [11, 23],
different mechanisms can be designed for different situations. For instance, an ad-bidding platform
is a highly successful market mechanism, though it differs significantly from an agentic platform.
5
```

</details>

---

### 6. Conclusion

**Score: 8.0/10**

**Evaluation:**
Score: 8/10

Reasoning:

Clarity and Writing Quality (2/2):
- The conclusion is well-written and articulate
- Clear language that effectively summarizes the paper's core contributions
- Smooth transition from findings to future research questions

Technical Depth (2/2):
- Demonstrates sophisticated understanding of AI agent ecosystems
- Introduces complex concepts like "balancing user agency and value"
- Articulates forward-looking research directions with technical nuance

Novelty and Originality (1.5/2):
- Proposes an innovative framework for thinking about human-AI collaboration
- Introduces the concept of "User-Centric AI in Agentic Enterprises"
- Suggests novel research questions that extend current understanding

Methodology Rigor (1.5/2):
- Presents core tenets as a methodological approach
- Outlines specific research questions with clear potential impact
- Shows structured thinking about future research directions

Evidence and Support (1/2):
- Limited direct evidence in conclusion section (which is typical)
- Builds on implied prior work in the paper
- Could benefit from slightly more explicit connection to earlier research

The conclusion effectively synthesizes the paper's contributions, highlights future research opportunities, and presents a compelling vision for human-AI collaboration. The minor deduction is primarily due to limited direct evidence presentation, which is inherent to conclusion sections.

<details>
<summary>üìÑ View Section Content (1443 characters)</summary>

```
Conclusion
The vision of people and AI working together to perform tasks can transition to reality with agents.
This transition requires taking conceptual notions of how such ecosystems of agents and people
will co-exist, and implementing them in specific ways. In this paper, we propose a framework to
reason about the core principles that such ecosystems should exemplify. We present these as tenets,
focused on balancing user agency and value as a core principle. Through these, we posit that future
agentic ecosystems can better serve users in enterprises for their decision-making.
Building on this foundation, several key research questions arise: How can we advance locally
privacy-preserving agent learning beyond current federated and distributed approaches? Specif-
ically, how can we enable fine-grained user control over which specific information is used for
general or targeted learning? Next, what strategies can create a diversity of risk-reward and quality-
price trade-offs in both processes and outputs to better meet varied user needs? Lastly, how can we
develop simulation environments that test different agent organizations and support diverse user
profiles for more effective platform evaluation and refinement? Addressing these questions will
pave the way from AI-Centric User towards User-Centric AI in Agentic Enterprises.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
Agentic Enterprise
111:11
```

</details>

---

### 7. References

**Score: 8.0/10**

**Evaluation:**
Score: 8/10

Reasoning:

Strengths (Positive Evaluation):
1. Clarity and Writing Quality (2/2):
- References are formatted consistently
- Uses standard academic citation style
- Includes diverse sources (academic journals, tech companies, preprints)
- Includes URLs and access dates for web sources

2. Technical Depth (2/2):
- References cover multiple domains: AI, psychology, decision-making, agent systems
- Sources span recent years (2019-2025)
- Includes both theoretical and applied research papers
- Incorporates academic and industry perspectives

3. Novelty and Originality (1.5/2):
- Several forward-looking/future-dated references suggest cutting-edge research
- Includes emerging topics like multi-agent AI systems
- Mix of theoretical foundations and contemporary research

4. Evidence and Support (1.5/2):
- Comprehensive range of sources
- Includes academic journals, arXiv preprints, and industry whitepapers
- References from reputable institutions (Google, Harvard, IEEE)

Minor Limitations:
- Some references appear speculative (future-dated)
- A few references might be hypothetical

The reference section demonstrates high scholarly rigor, comprehensive coverage, and thoughtful source selection, warranting a strong 8/10 score.

<details>
<summary>üìÑ View Section Content (2454 characters)</summary>

```
References
[1] Deepak Bhaskar Acharya, Karthigeyan Kuppan, and B Divya. 2025. Agentic AI: Autonomous Intelligence for Complex
Goals‚ÄìA Comprehensive Survey. IEEE Access (2025).
[2] George A Akerlof. 1978. The Market for ‚ÄúLemons‚Äù: Quality Uncertainty and the Market Mechanism. In Uncertainty in
Economics. Elsevier, 235‚Äì251.
[3] Anthropic. 2025. How We Built Our Multi-Agent Research System. https://www.anthropic.com/engineering/built-
multi-agent-research-system Accessed: 2025-06-14.
[4] Albert Bandura. 2006. Toward a Psychology of Human Agency. Perspectives on Psychological Science 1, 2 (2006),
164‚Äì180.
[5] Dibyanayan Bandyopadhyay, Soham Bhattacharjee, and Asif Ekbal. 2025. Thinking Machines: A Survey of LLM Based
Reasoning Strategies. arXiv preprint arXiv:2503.10814 (2025).
[6] Max H Bazerman and Don A Moore. 2012. Judgment in Managerial Decision Making. John Wiley & Sons.
[7] Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda,
Demetri Terzopoulos, Yejin Choi, et al. 2024. Agent AI: Surveying the Horizons of Multimodal Interaction. arXiv
preprint arXiv:2401.03568 (2024).
[8] Mohamed Amine Ferrag, Norbert Tihanyi, and Merouane Debbah. 2025. From LLM Reasoning to Autonomous AI
Agents: A Comprehensive Review. arXiv preprint arXiv:2504.19678 (2025).
[9] Google LLC. 2024. Prompting Guide 101. https://services.google.com/fh/files/misc/gemini-for-google-workspace-
prompting-guide-101.pdf
[10] Google LLC. 2025. Google Agentspace Enterprise Overview. https://cloud.google.com/agentspace/agentspace-enterprise/
docs/overview Accessed: 2025-06-14.
[11] Oliver D Hart. 1983. The Market Mechanism as an Incentive Scheme. The Bell Journal of Economics (1983), 366‚Äì382.
[12] Chad A Hartnell, Amy Yi Ou, and Angelo Kinicki. 2011. Organizational Culture and Organizational Effectiveness:
A Meta-analytic Investigation of the Competing Values Framework‚Äôs Theoretical Suppositions. Journal of Applied
Psychology 96, 4 (2011), 677.
[13] Harvard Business Review. 2023. The New Human-Machine Relationship. https://store.hbr.org/product/the-new-
human-machine-relationship/R2302B Accessed: 2025-06-14.
[14] Jeffrey Heer. 2019. Agency Plus Automation: Designing Artificial Intelligence into Interactive Systems. Proceedings of
the National Academy of Sciences 116, 6 (2019), 1844‚Äì1850.
[15] E Tory Higgins. 1998. Promotion and Prevention: Regulatory Focus as a Motivational Principle. In Advances in
```

</details>

---

### 8. Experimental Social Psychology. Vol. 30. Elsevier, 1‚Äì46.

**Score: 5.0/10**

**Evaluation:**
Score: 5/10

Reasoning:
This section appears to be a reference list or bibliography from an academic paper, likely related to AI, agent systems, and computational research. While the content itself is not a substantive research section, I'll evaluate it based on the available references:

Clarity and Writing Quality (1/2):
- The references are standard academic citations
- Proper formatting and consistent style
- No direct writing content to evaluate deeply

Technical Depth (1/2):
- Diverse references covering AI, agent systems, machine learning
- Mix of conferences, journals, and preprint sources
- References span from 2002 to projected 2025 publications
- Covers emerging topics like AI agents, LLM planning

Novelty and Originality (1/2):
- Some interesting emerging research topics
- References cutting-edge AI research areas
- Mix of technical and conceptual papers

Methodology Rigor (1/2):
- References from reputable conferences/journals
- Includes technical reports and academic publications
- Diverse methodological approaches represented

Evidence and Support (1/2):
- Multiple citations supporting emerging AI research themes
- References from leading institutions and researchers
- Mix of empirical and theoretical work

The middling score reflects that this is a reference list, not a research section, making comprehensive evaluation challenging. The references suggest a sophisticated, contemporary research exploration of AI agent systems and computational methodologies.

<details>
<summary>üìÑ View Section Content (5988 characters)</summary>

```
Experimental Social Psychology. Vol. 30. Elsevier, 1‚Äì46.
[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic Models. Advances in Neural
Information Processing Systems 33 (2020), 6840‚Äì6851.
[17] Qiuyuan Huang, Naoki Wake, Bidipta Sarkar, Zane Durante, Ran Gong, Rohan Taori, Yusuke Noda, Demetri Terzopoulos,
Noboru Kuno, Ade Famoti, et al. 2024. Position Paper: Agent AI Towards a Holistic Intelligence. arXiv preprint
arXiv:2403.00833 (2024).
[18] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and
Enhong Chen. 2024. Understanding the Planning of LLM agents: A Survey. arXiv preprint arXiv:2402.02716 (2024).
[19] Laurie Hughes, Yogesh K Dwivedi, Tegwen Malik, Mazen Shawosh, Mousa Ahmed Albashrawi, Il Jeon, Vincent Dutot,
Mandanna Appanderanda, Tom Crick, Rahul De‚Äô, et al. 2025. AI Agents and Agentic Systems: a Multi-Expert Analysis.
Journal of Computer Information Systems (2025), 1‚Äì29.
[20] Sebastian Krakowski. 2025. Human-AI Agency in the Age of Generative AI. Information and Organization 35, 1 (2025),
100560.
[21] Vishwajeet Kumar, Yash Gupta, Saneem Chemmengath, Jaydeep Sen, Soumen Chakrabarti, Samarth Bharadwaj, and
Feifei Pan. 2023. Multi-Row, Multi-Span Distant Supervision For Table+Text Question Answering. In Proceedings of
the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan
Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 8080‚Äì8094.
doi:10.18653/v1/2023.acl-long.449
[22] Doris L√§pple and Bradford L Barham. 2019. How do Learning Ability, Advice from Experts and Peers Shape Decision
Making? Journal of Behavioral and Experimental Economics 80 (2019), 92‚Äì107.
[23] Roger B Myerson. 2008. Perspectives on Mechanism Design in Economic Theory. American Economic Review 98, 3
(2008), 586‚Äì603.
[24] Arpit Narechania, Alex Endert, and Atanu R Sinha. 2025. Guidance Source Matters: How Guidance from AI, Expert, or
a Group of Analysts Impacts Visual Data Preparation and Analysis. In Proceedings of the 30th International Conference
on Intelligent User Interfaces. 789‚Äì809.
[25] Giang Nguyen, Ivan Brugere, Shubham Sharma, Sanjay Kariyappa, Anh Totti Nguyen, and Freddy Lecue. 2024.
Interpretable Table Question Answering via Plans of Atomic Table Transformations. https://openreview.net/forum?
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
111:12
Narechania, Endert, and Sinha
id=mDV36U4d6u
[26] OpenAI. 2024. AI in the Enterprise. Technical Report. OpenAI. https://cdn.openai.com/business-guides-and-resources/
ai-in-the-enterprise.pdf Accessed: 2025-06-17.
[27] OpenAI. 2025. A Practical Guide to Building Agents. https://cdn.openai.com/business-guides-and-resources/a-practical-
guide-to-building-agents.pdf Accessed: 2025-06-14.
[28] Ansh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson Denison, Danny Hernandez, Esin Durmus,
Evan Hubinger, Jackson Kernion, KamilÀôe Luko≈°i¬ØutÀôe, et al. 2023. Question Decomposition Improves the Faithfulness of
Model-Generated Reasoning. arXiv preprint arXiv:2307.11768 (2023).
[29] Ashay Satav. 2025. Enterprise API & Platform Strategy in the Era of Agentic AI. Journal of Computer Science and
Technology Studies 7, 1 (2025), 380‚Äì385.
[30] Yijia Shao, Humishka Zope, Yucheng Jiang, Jiaxin Pei, David Nguyen, Erik Brynjolfsson, and Diyi Yang. 2025. Future
of Work with AI Agents: Auditing Automation and Augmentation Potential across the US Workforce. arXiv preprint
arXiv:2506.06576 (2025).
[31] Shavit, Yonadav and Agarwal, Sandhini and Brundage, Miles and Adler, Steven and O‚ÄôKeefe, Cullen and Campbell,
Rosie and Lee, Teddy and Mishkin, Pamela and Eloundou, Tyna and Hickey, Alan and others. 2023. Practices for
Governing Agentic AI Systems. Technical Report. OpenAI. https://cdn.openai.com/papers/practices-for-governing-
agentic-ai-systems.pdf Accessed: 2025-06-17.
[32] Paschal Sheeran. 2002. Intention‚ÄîBehavior Relations: a Conceptual and Empirical Review. European Review of Social
Psychology 12, 1 (2002), 1‚Äì36.
[33] Chufan Shi, Yixuan Su, Cheng Yang, Yujiu Yang, and Deng Cai. 2023. Specialist or Generalist? Instruction Tuning for
Specific NLP Tasks. arXiv preprint arXiv:2310.15326 (2023).
[34] Ben Shneiderman. 2022. Human-Centered AI. Oxford University Press.
[35] George J Stigler. 1974. Free Riders and Collective Action: An Appendix to Theories of Economic Regulation. The Bell
Journal of Economics and Management Science (1974), 359‚Äì365.
[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. 2017. Attention is All You Need. Advances in Neural Information Processing Systems 30 (2017).
[37] Dakuo Wang, Elizabeth Churchill, Pattie Maes, Xiangmin Fan, Ben Shneiderman, Yuanchun Shi, and Qianying Wang.
2020. From Human-Human Collaboration to Human-AI Collaboration: Designing AI Systems that Can Work Together
with People. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems. 1‚Äì6.
[38] Hui Wei, Zihao Zhang, Shenghua He, Tian Xia, Shijia Pan, and Fei Liu. 2025. PlanGenLLMs: A Modern Survey of LLM
Planning Capabilities. arXiv preprint arXiv:2502.11221 (2025).
[39] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree
of Thoughts: Deliberate Problem Solving with Large Language Models. Advances in Neural Information Processing
Systems 36 (2023), 11809‚Äì11822.
[40] Kun Zhang, Jiali Zeng, Fandong Meng, Yuanzhuo Wang, Shiqi Sun, Long Bai, Huawei Shen, and Jie Zhou. 2024.
Tree-of-Reasoning Question Decomposition for Complex Question Answering with Large Language Models Authors.
In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 19560‚Äì19568.
Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
```

</details>

---

## Technical Details

- **PDF Path**: `papers/Bottom-1-2506.22893v1.pdf`
- **Processing Configuration**:
  - max_chunk_size: 4000
  - min_chunk_size: 100
  - delay_between_requests: 1.0

*Analysis generated by Research Paper Scorer v0.1.0*
