# Research Paper Analysis: Top1-2506.02064v1.pdf

## Summary

| Metric | Value |
|--------|-------|
| **Paper** | Top1-2506.02064v1.pdf |
| **Analysis Date** | 2025-07-11 23:35:58 |
| **Overall Score** | **7.00/10** |
| **Sections Analyzed** | 21 |
| **Processing Time** | 187.37 seconds |
| **Model Used** | claude-3-5-haiku-20241022 |

## Score Distribution

- **Abstract**: 8.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œ
- **Evaluation Frameworks Systematically Privilege Narrow Technical Metrics While**: 6.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œâ¬œâ¬œ
- **Ntroduction**: 7.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œâ¬œ
- **Evaluation Frameworks Remain Noticeably Imbalanced. We Define Measurement Imbalance As The**: 8.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œ
- **Evaluation Tools Required To Validate The Productivity And Efficiency Claims. Technical Metrics, While**: 8.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œ
- **Evaluations. (2) Illustrating Deployment Consequences With Real-World Examples. We Document**: 8.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œ
- **Methodology**: 8.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œ
- **Limitations. Our Sample May Not Be Representative Of Industry Practices Due To Proprietary Evaluation**: 2.0/10 ğŸŸ©ğŸŸ©â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ
- **Methods. Additionally, The Rapid Evolution Of Agentic Ai Systems Means Some Recent Developments**: 8.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œ
- **Evaluation Metrics.**: 8.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œ
- **Evaluation With Real-World Success, We Propose A Framework That Balances These Dimensions, Supports**: 9.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œ
- **Evaluation That Is Minimum Viable Testing At Early Stages, And Deep Analysis Before Full Deployment.**: 9.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œ
- **Limitations.**: 5.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œâ¬œâ¬œâ¬œ
- **Evaluation Forms The Foundation For Trust, Effectiveness, And Accountability In Ai Systems. This**: 3.0/10 ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ
- **Conclusion And Call To Action**: 7.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œâ¬œ
- **Evaluation Must Evolve.**: 7.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œâ¬œ
- **References**: 7.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œâ¬œ
- **Methods In Natural Language Processing, 2024.**: 6.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œâ¬œâ¬œ
- **Methods And Programs In Biomedicine Update, 2024.**: 8.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œ
- **Evaluation**: 6.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œâ¬œâ¬œ
- **Acknowledgment, Multiple Evaluation Methods, Reproducibility**: 9.0/10 ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œ

---

## Detailed Section Analysis

### 1. Abstract

**Score: 8.0/10**

**Evaluation:**
Score: 8/10

Reasoning:

Clarity and Writing Quality (2/2):
The abstract is exceptionally clear and well-structured. It presents a complex argument in a concise, accessible manner. The language is precise and flows logically, effectively communicating the core research focus and findings.

Technical Depth (2/2):
The abstract demonstrates strong technical depth by providing specific quantitative insights (84 papers reviewed, percentage breakdowns of evaluation metrics). It shows a sophisticated understanding of the current landscape of AI system evaluations and highlights a critical methodological gap.

Novelty and Originality (1.5/2):
The research presents a novel critique of current AI evaluation practices, challenging prevailing industry narratives. The systematic review of evaluation metrics across different sectors and the emphasis on the disconnect between technical benchmarks and real-world implementation is particularly innovative.

Methodology Rigor (1.5/2):
The abstract hints at a robust methodology with a systematic review of 84 papers, spanning multiple sectors. The breadth of analysis (technical, human-centered, safety, economic assessments) suggests a comprehensive research approach.

Evidence and Support (1/2):
While the abstract promises evidence from multiple sectors, the specific details are not elaborated. More explicit references to concrete examples would strengthen the argument.

The abstract effectively sets up the research question, highlights the research's significance, and provides a clear preview of the paper's findings and contribution to the field of AI system evaluation.

<details>
<summary>ğŸ“„ View Section Content (1126 characters)</summary>

```
Abstract
As industry reports claim agentic AI systems deliver double-digit productivity gains
and multi-trillion dollar economic potential, the validity of these claims has become
critical for investment decisions, regulatory policy, and responsible technology
adoption. However, this paper demonstrates that current evaluation practices for
agentic AI systems exhibit a systemic imbalance that calls into question prevailing
industry productivity claims. Our systematic review of 84 papers (2023â€“2025)
reveals an evaluation imbalance where technical metrics dominate assessments
(83%), while human-centered (30%), safety (53%), and economic assessments
(30%) remain peripheral, with only 15% incorporating both technical and human
dimensions. This measurement gap creates a fundamental disconnect between
benchmark success and deployment value. We present evidence from healthcare,
finance, and retail sectors where systems excelling on technical metrics failed in
real-world implementation due to unmeasured human, temporal, and contextual
factors. Our position is not against agentic AIâ€™s potential, but rather that current
```

</details>

---

### 2. Evaluation Frameworks Systematically Privilege Narrow Technical Metrics While

**Score: 6.0/10**

**Evaluation:**
Score: 6/10

Reasoning:

Clarity and Writing Quality (1.5/2):
- The text is concise and articulate
- Clear argument structure about the limitations of current evaluation frameworks
- Slightly abstract language, but generally comprehensible

Technical Depth (1/2):
- Lacks specific details about the proposed "four-axis evaluation model"
- Presents a high-level critique without substantive technical elaboration
- More context would strengthen the technical substance

Novelty and Originality (1.5/2):
- Interesting critique of benchmark-driven optimization
- Calls for a paradigm shift in evaluation frameworks
- Highlights an important meta-level concern in system design

Methodology Rigor (1/2):
- No detailed methodology presented
- Merely proposes a conceptual framework without concrete implementation details
- Lacks specifics on how the four-axis model would be constructed

Evidence and Support (1/2):
- No explicit citations or empirical evidence
- Makes broad claims without immediate substantiation
- Relies on rhetorical persuasion rather than detailed proof

The section shows promise in identifying a critical issue in system evaluation but would benefit from more technical specificity, concrete methodology, and supporting evidence. It reads more like a provocative position statement than a rigorous research argument.

<details>
<summary>ğŸ“„ View Section Content (469 characters)</summary>

```
evaluation frameworks systematically privilege narrow technical metrics while
neglecting dimensions critical to real-world success. We propose a balanced four-
axis evaluation model and call on the community to lead this paradigm shift because
benchmark-driven optimization shapes what we build. By redefining evaluation
practices, we can better align industry claims with deployment realities and ensure
responsible scaling of agentic systems in high-stakes domains.
1
```

</details>

---

### 3. Ntroduction

**Score: 7.0/10**

**Evaluation:**
Score: 7

Reasoning:

Clarity and Writing Quality (1.5/2):
- The text is generally clear and well-structured
- Defines agentic AI systems with a concise, systematic approach
- Uses numbered points to break down characteristics effectively

Technical Depth (1.5/2):
- Provides a substantive overview of agentic AI system characteristics
- Includes citations to support each characteristic
- Demonstrates a good technical understanding of the domain

Novelty and Originality (1/2):
- The description is standard but solid
- Does not present groundbreaking new insights
- Provides a conventional but accurate characterization of agentic AI systems

Methodology Rigor (1/2):
- Limited methodology discussion in this introductory section
- Citations suggest underlying research support
- More methodological details would be beneficial

Evidence and Support (1/2):
- Uses multiple citations to support claims
- Citations appear to be from reputable sources
- Could benefit from more specific references or examples

Observations:
- The section seems to be the beginning of an introduction, cut off mid-sentence
- Promising start that sets up the context for agentic AI systems
- Would benefit from completing the thought and providing more context

The score reflects a solid, technically sound introduction with room for more depth and originality.

<details>
<summary>ğŸ“„ View Section Content (504 characters)</summary>

```
Introduction
Agentic AI systems are characterized by: (1) goal-directed behavior with the ability to decompose
complex objectives into manageable subtasks [20]; (2) environmental awareness and adaptability to
changing conditions [7]; (3) tool utilization, where agents strategically leverage external resources to
accomplish tasks [2]; and (4) autonomous decision-making with limited human intervention [34].
These systems are rapidly moving from research labs to critical real-world deployments, yet our
```

</details>

---

### 4. Evaluation Frameworks Remain Noticeably Imbalanced. We Define Measurement Imbalance As The

**Score: 8.0/10**

**Evaluation:**
Score: 8/10

Reasoning:

Clarity and Writing Quality (2/2):
The section is very well-written, with clear prose and a logical flow. The authors effectively communicate their central argument about evaluation framework imbalances. The writing is academic yet accessible, with a good balance of technical language and explanatory context.

Technical Depth (2/2):
The section demonstrates substantial technical depth. It provides a nuanced critique of current AI evaluation frameworks, citing specific benchmarks (MLAgentBench, ML-Bench, SUPER), and offering concrete examples from various domains like healthcare and trading. The authors show a sophisticated understanding of both technical metrics and broader evaluation challenges.

Novelty and Originality (1.5/2):
The argument about the imbalance in AI evaluation frameworks is quite novel. The systematic critique of how current benchmarks overemphasize technical performance while neglecting human-centered factors is an original and important contribution. The systematic review of 84 papers provides empirical weight to their argument.

Methodology Rigor (1/2):
While the methodology is not fully detailed in this section, the authors hint at a systematic literature review. More explicit explanation of their review methodology would strengthen this aspect.

Evidence and Support (1.5/2):
The section is well-supported with multiple citations across disciplines (HCI, machine learning, industry reports). The concrete example of healthcare diagnostic agents illustrates their point effectively. The statistical breakdown of their literature review (83% technical vs. 30% human-centered evaluation) provides strong evidential support.

Minor critiques include a small typo (âˆ—corrspeonding) and the need for more methodological transparency. Overall, this is a high-quality section that makes a compelling argument about the current limitations in AI evaluation frameworks.

<details>
<summary>ğŸ“„ View Section Content (2697 characters)</summary>

```
evaluation frameworks remain noticeably imbalanced. We define measurement imbalance as the
systematic bias in evaluation frameworks that privilege easily quantifiable technical metrics while
neglect dimensions critical to real-world deployment success; especially human-centered factors,
temporal stability, and contextual fit. This imbalance creates a fundamental misalignment between
what we measure and what determines system value. For instance, healthcare diagnostic agents
âˆ—corrspeonding author: kjafari@stanford.edu
â€ denotes equal contribution
Preprint. Under review.
achieving 95% benchmark accuracy have been relegated to limited advisory roles post-deployment
due to unmeasured trust and workflow integration issues [17].
In 2024, agentic AI systems were deployed across sectors such as: clinical triage [3, 6], automated
trading [46, 44], customer service [13, 33], and internal software debugging [16, 32]. Firms report
double-digit productivity gains from these deployments, citing speed, cost savings, and increased
decision accuracy [28]. These claims are bolstered by benchmark results showcasing near-human or
superhuman performance on standard tasks [18, 4].
Benchmarks have long driven progress in machine learning, from ImageNet [10] to GLUE [42] and
HELM [25]. In the agentic AI space, MLAgentBench [18], ML-Bench [38], and SUPER [4] evaluate
task success, efficiency, and end-to-end execution using predefined scripts or repository-grounded
tasks. PlanBench [39] introduces symbolic validation for plan structure, while VisualWebArena [23]
focuses on multimodal agents in web environments. These frameworks provide critical infrastructure
for measuring technical competence but rarely assess how agents integrate into human workflows or
operate over time.
Meanwhile, HCI and social computing scholars have long emphasized trust, usability, and alignment
as critical success factors in AI deployment [37, 29]. Recent agentic AI work includes TrAAIT [36],
a validated instrument for clinician trust in AI systems; [30] highlights transparency and observability
needs; [11], propose a multi-axis framework for agent intelligence, adaptivity, and civility. How-
ever, these human-centered approaches remain fragmented with limited integration into standard
benchmarks or industry practice.
This disconnect between technical and sociotechnical evaluation is confirmed by our systematic review
of 84 academic and industry papers from 2023â€“2025. Among these works, technical performance
was strongly represented (83%), while human-centered evaluation appeared in 30%, and both in only
15%.
Despite the widespread enthusiasm and rapid adoption, we currently lack the multidimensional
```

</details>

---

### 5. Evaluation Tools Required To Validate The Productivity And Efficiency Claims. Technical Metrics, While

**Score: 8.0/10**

**Evaluation:**
Score: 8/10

Reasoning:

Clarity and Writing Quality (2/2):
The section is exceptionally well-written, with clear, sophisticated language that communicates complex ideas effectively. The prose is articulate and flows logically, presenting a sophisticated argument about evaluation frameworks for agentic AI systems.

Technical Depth (2/2):
The content demonstrates significant technical depth by critically analyzing current evaluation methodologies. The authors show nuanced understanding of the limitations of technical metrics and the broader implications for AI system deployment. The discussion goes beyond surface-level observations to provide substantive critique.

Novelty and Originality (1.5/2):
The section presents a novel perspective on AI evaluation, challenging existing benchmark-driven approaches. The argument that current metrics are insufficient and potentially misleading is an important contribution. The meta-analysis of 84 publications to quantify metric imbalance suggests an original research approach.

Methodology Rigor (1.5/2):
While only a preview of the full methodology, the section outlines a rigorous approach to examining evaluation frameworks. The authors clearly state their intent to provide a quantitative diagnosis and suggest a comprehensive approach to understanding AI system performance.

Evidence and Support (1/2):
The section promises empirical evidence (meta-analysis of 84 publications) but doesn't provide specific details in this excerpt. While the argument is compelling, more concrete evidence would strengthen the claims.

Slight deductions are made for the incomplete presentation of evidence and the somewhat theoretical nature of the argument at this stage. However, the section is extremely promising and demonstrates high-quality scholarly writing about an important emerging issue in AI evaluation.

<details>
<summary>ğŸ“„ View Section Content (1946 characters)</summary>

```
evaluation tools required to validate the productivity and efficiency claims. Technical metrics, while
necessary, capture only a narrow slice of what determines success in real-world deployments. As
agentic systems gain more autonomy and become embedded in organizational workflows, this
measurement imbalance threatens to create a new wave of mismatched expectations, misallocated
resources, and poorly understood risks.
We argue that the prevailing evaluation frameworks for agentic AI are largely incomplete,
systematically privileging technical performance metrics while underrepresenting critical
dimensions such as human-centered interaction, longitudinal behavior, safety, and contextual
fit. As a result, the industryâ€™s current productivity claims are premature and, in many cases,
can be misleading.
We do not argue against the value of agentic systems, nor do we dispute their emerging capabilities.
Instead, we focus on the instruments of validation; the metrics, benchmarks, and evaluation practices
that translate system behavior into evidence. Our central claim is that these instruments are misaligned
with deployment reality, and that evaluation itself must evolve if we are to responsibly scale agentic
AI in high-stakes domains.
Benchmark-driven optimization has long been the engine of ML progress, but when benchmarks miss
key dimensions of impact, they also drive blind spots. What we measure shapes what we build. As
venues for foundational work in LLMs, agent architectures, and evaluation benchmarks, leading AI
conferences are well-positioned to catalyze this needed shift toward more comprehensive evaluation
practices
This paper takes a position grounded in both empirical evidence and practical urgency, offering the
following contributions: (1) A quantitative diagnosis of the current imbalance. Our meta-analysis of
84 publications quantifies the overrepresentation of technical metrics in both academic and industry
```

</details>

---

### 6. Evaluations. (2) Illustrating Deployment Consequences With Real-World Examples. We Document

**Score: 8.0/10**

**Evaluation:**
Score: 8/10

Reasoning:

1. **Clarity and Writing Quality** (2/2): 
- The text is exceptionally well-written, clear, and articulate
- Concepts are explained with precision and intellectual sophistication
- Language is professional and engaging, making complex ideas accessible

2. **Technical Depth** (2/2):
- Demonstrates deep technical understanding of AI evaluation challenges
- Provides concrete examples (MLAgentBench, TrAAIT) to illustrate conceptual points
- Articulates nuanced insights about measurement limitations in AI systems

3. **Novelty and Originality** (1.5/2):
- Presents a novel critique of current AI evaluation methodologies
- Introduces an innovative multi-dimensional framework for assessment
- Highlights critical gaps between technical benchmarks and real-world performance

4. **Methodology Rigor** (1.5/2):
- Proposes a structured approach to addressing evaluation limitations
- Outlines a comprehensive four-axis model for assessment
- Anticipates and addresses potential counterarguments systematically

5. **Evidence and Support** (1/2):
- References specific benchmarks and research instruments
- Suggests cross-domain implications
- Could benefit from more quantitative evidence/concrete case studies

The section is compelling, intellectually robust, and makes a strong argument for a more holistic approach to AI system evaluation. Its primary strength lies in identifying critical measurement blind spots and proposing a constructive path forward.

Minor improvements could include more empirical data and specific examples of deployment failures to further strengthen the argument.

<details>
<summary>ğŸ“„ View Section Content (3818 characters)</summary>

```
evaluations. (2) Illustrating deployment consequences with real-world examples. We document
how overreliance on narrow evaluation metrics led to adoption failure or unexpected business losses
in healthcare, financial services, and retail deployments. (3) A new theoretical framework for
balanced evaluation. We introduce a four-axis model; technical, human, temporal, and contextual, and
propose a formal structure for integrating multiple metric classes across domains. (4) Anticipation
and rebuttal of key counterarguments. We address recurring counterarguments including: that
2
human-centered metrics are too subjective; that safety and governance belong to regulators, not ML
researchers and thus our of scope; that adding more metrics slows interaction and stifles innovation;
that one size cannot fit all domains, and universal frameworks are inherently vague. (5) Actionable
recommendations for researchers, industry, and policymakers. We offer a roadmap for closing the
measurement gap through benchmark design, instrument development, deployment best practices,
and regulatory levers.
By grounding what we count as evidence, this paper aims to reset the conversation on agentic AI
efficiency, not to constrain innovation, but to ensure that the claims we make reflect the systems we
actually build (see Appendix A.4 for formal definitions).
2
Why Evaluation Matters
Unlike classical predictive models, Agentic AI value hinges on sustained interaction with people,
data sources, and other agents across time and domains. As these systems rapidly transition from
research to deployment in various sectors such as healthcare, finance, and retail, the gap between how
we evaluate them and how they actually create value becomes critical.
MLAgentBench [18] is an example of todayâ€™s technical focus: agents are scored on Pass@k success,
token efficiency, and wall-clock time while running scripted ML experiments. The benchmark yields
crisp, automatable numbers but tells us nothing about how well an agent fits a collaborative workflow
or whether its behavior degrades after extended autonomous operation.
By contrast, TrAAIT, a clinician-trust instrument for AI decision support, captures perceived useful-
ness, ease of integration, and social influence through validated survey scales [36]. TrAAITâ€™s scores
directly predict real-world adoption but is rarely included in research papers. The field celebrates
technical benchmark leaderboards while few report trust or usability scores, reflecting a marked
misalignment between what we measure and what determines deployment success.
Systems that ace technical benchmarks can still be rejected by end-users if trust, workflow compati-
bility, or explanation quality are weak (adoption risk). Point-in-time accuracy masks drift, emergent
behavior, and edge-case failure modes that surface only after deployment, creating potential for harm
that remains invisible to current evaluation frameworks (safety blind spots). ROI projections based
solely on speed or accuracy overlook human oversight costs and adaptation lags, leading to costly
rollout reversals and failed investments (misjudged productivity).
When optimize for the same narrow slice of metrics, the research community and industry alike are
effectively flying blind on the dimensions that determine real-world success. To unlock the genuine
productivity potential of agentic AI and to avoid a wave of failed deployments and eroded trust,
we must rebalance evaluation toward a multidimensional, longitudinal, and context-aware basis.
The remainder of this paper supplies the evidence, framework, and agenda required to make that
shift, drawing on instances across sectors where measurement imbalance has already led to costly
consequences.
3
Meta-Analysis: A Quantitative View of the Evaluation Gap
3.1
```

</details>

---

### 7. Methodology

**Score: 8.0/10**

**Evaluation:**
Score: 8/10

Reasoning:

Clarity and Writing Quality (2/2):
The methodology section is well-written, clear, and concise. The description of the search process, inclusion criteria, and coding scheme is systematic and easy to understand. The language is professional and technical.

Technical Depth (2/2):
The methodology demonstrates strong technical depth. The authors provide a detailed explanation of their systematic review process, including:
- Specific search terms
- Clear inclusion criteria
- Independent screening process
- Inter-reviewer agreement
- Development of a comprehensive four-category metric scheme

Novelty and Originality (1/2):
The methodology shows some novelty in the four-category metric scheme, which covers technical, human-centered, safety, and economic aspects. However, the systematic review approach itself is relatively standard in academic research.

Methodology Rigor (2/2):
The methodology is highly rigorous:
- Systematic search across multiple databases
- Clear inclusion/exclusion criteria
- Independent screening with high agreement (85%)
- Iterative process for developing metric categories
- Inter-reliability testing
- Binary coding approach
- Transparency through available codebook and coded records

Evidence and Support (1/2):
The section references an established AI evaluation metrics source [43] and mentions pilot coding. However, more details about the pilot coding process and the specific reference would strengthen the evidence.

Overall, this is a strong, well-structured methodology section that provides a comprehensive and transparent approach to systematic review and metric categorization.

<details>
<summary>ğŸ“„ View Section Content (1383 characters)</summary>

```
Methodology
We conducted systematic searches across arXiv and Google Scholar. Search terms included combina-
tions of â€œagentic AIâ€, â€œAI agentsâ€, â€œLLM agentsâ€, â€œevaluationâ€, â€œbenchmarkâ€, â€œassessmentâ€, and
â€œmetricâ€. Papers were included if (1) published between January 2023 and April 2025, (2) focused
on agentic or LLM-based systems as defined in Section 1, (3) available in English, (4) proposed or
applied evaluation methodology and metrics.
Two reviewers independently screened 138 papers (with 85% agreement), with disagreements resolved
by a third reviewer. Out of 138 papers, 84 of them met our inclusion criteria. We adopted a four-
category scheme that was developed through an iterative process based on established AI evaluation
metrics [43], pilot coding of 25 papers, and inter-reliability testing. These four metric categories are:
Technical such as task success rate, accuracy, latency, throughput, human-centered such as trust,
usability, collaboration, workflow integration, safety and governance such as robustness, compliance,
explainability, alignment, and economic impact such as ROI, cost savings, value realization. We
3
also looked at evaluation quality to make sure the methodology details are described in the paper.
Coding was binary per category. The full codebook is available in Appendix A.1, and coded records
are available in the accompanying PDF file.
```

</details>

---

### 8. Limitations. Our Sample May Not Be Representative Of Industry Practices Due To Proprietary Evaluation

**Score: 2.0/10**

**Evaluation:**
Score: 2

Reasoning:
This section fails to meet basic academic writing standards and provides minimal useful information:

1. **Clarity and Writing Quality** (0.5/2):
- The text is extremely brief and lacks substantive explanation
- It reads more like a sentence fragment than a properly constructed paragraph
- Grammar and structure are poor

2. **Technical Depth** (0.5/2):
- No meaningful technical details are provided
- No elaboration on what "proprietary evaluation" means
- No context about sampling methodology or potential biases

3. **Novelty and Originality** (0.5/2):
- No original insights are presented
- The statement is generic and could apply to almost any research study

4. **Methodology Rigor** (0.5/2):
- No explanation of sampling methodology
- No discussion of how the potential non-representativeness might impact results
- No suggestions for mitigating sampling limitations

5. **Evidence and Support** (0/2):
- No evidence is provided to support the limitation claim
- No citations or additional context
- No quantitative or qualitative details about sampling challenges

The section is critically underdeveloped and fails to provide meaningful insights into the study's limitations. A proper limitations section should critically analyze potential weaknesses, provide context, and discuss potential impacts on research validity.

Recommendation: Completely rewrite the section with detailed, specific explanations of sampling limitations, potential biases, and their implications for the research findings.

<details>
<summary>ğŸ“„ View Section Content (101 characters)</summary>

```
Limitations. Our sample may not be representative of industry practices due to proprietary evaluation
```

</details>

---

### 9. Methods. Additionally, The Rapid Evolution Of Agentic Ai Systems Means Some Recent Developments

**Score: 8.0/10**

**Evaluation:**
Score: 8/10

Reasoning:

Strengths (Positive Scoring Factors):
1. Clarity and Writing Quality (2/2):
- Exceptionally clear and articulate prose
- Well-structured arguments with precise language
- Coherent narrative flow between subsections

2. Technical Depth (2/2):
- Comprehensive analysis of AI system evaluation metrics
- Nuanced exploration of limitations in current evaluation approaches
- Provides concrete examples across multiple domains (healthcare, finance)

3. Novelty and Originality (1.5/2):
- Offers critical insights into systemic gaps in AI system assessment
- Highlights important but often overlooked evaluation dimensions
- Challenges prevailing technical benchmarking paradigms

4. Methodology Rigor (1.5/2):
- Systematic data analysis of research paper metrics
- Multi-domain case studies demonstrating theoretical arguments
- Clear presentation of research findings

5. Evidence and Support (1/2):
- Multiple citations supporting claims
- Quantitative data from research studies
- Real-world case studies

Minor Areas for Improvement:
- Could benefit from more explicit recommendations for improving evaluation frameworks
- Some sections feel slightly fragmented

The section provides an insightful, well-researched critique of current AI system evaluation practices, demonstrating sophisticated analytical thinking about technological assessment methodologies.

<details>
<summary>ğŸ“„ View Section Content (4875 characters)</summary>

```
methods. Additionally, the rapid evolution of agentic AI systems means some recent developments
may not be captured here.
3.2
Data Analysis
In our analysis of qualified papers technical metrics dominated (83%), while human-centered and
economic impact metrics were less common (both 30%). Notably, only 15% of papers included both
technical and human-centered metrics, and a mere 5% incorporated any longitudinal dimension (de-
tailed distributions in Appendix A.3).
Academic papers were more likely to emphasize standardized technical benchmarks (96% vs. 87%),
while industry publications more frequently included economic (39% vs. 14%) and human-centered
(57% vs. 34%) metrics. However, only a small minority in either group employed multidimensional
or longitudinal evaluation strategies.
Technical metrics were also the most standardized, with 72% referencing formal benchmarks. Human-
centered and economic metrics, by contrast, were mostly ad hoc or qualitative, with only 18% and
12% respectively using validated instruments. Safety and governance metrics fell in between, often
borrowing from emerging regulatory standards.
These patterns can show a bias toward metrics that are automatable, replicable, and leaderboard-
friendly. While useful for measuring discrete capabilities, they ignore the dimensions that determine
real-world value such as human alignment, safety resilience, and temporal stability. Consequently,
the evidence base becomes structurally unbalanced, prioritizing narrow optimization over deployment
risk.
4
Case-Study Accounts: When Metrics Fail
Quantitative analysis alone cannot reveal the human and organizational consequences of evaluation
gaps. This section presents real-world deployments across healthcare, finance, and retail where
systems that performed strongly on benchmark metrics failed to deliver anticipated value. In each
case, critical dimensionsâ€”trust calibration, workflow integration, temporal stability, and contextual
fitâ€”were either unmeasured or misrepresented in initial evaluation, leading to adoption breakdowns
or business losses.
4.1
Healthcare: Diagnostic Support Systems that Failed to Integrate
The healthcare sector provides evidence that technical accuracy does not guarantee real-world success.
AI diagnostic agents deployed across hospitals often demonstrate high performance in controlled
tests; typically 90â€“95% diagnostic accuracy and superior documentation completeness compared to
junior residents [22].
Based on these benchmarks, institutions projected significant workload reduction and multi-million
dollar savings [31]. Yet post-deployment assessments frequently report adoption challenges. Although
the healthcare sector generates over one-third of global data, only an estimated 3% is effectively used
in live deployments [35]. A Turing Institute study found that medical triage systems with strong lab
metrics made â€œlittle to no differenceâ€ in clinical workflows [17].
Misalignment stemmed from evaluations conducted in environments that did not mirror real-world
complexity or workflow patterns [36]. For instance, DoctorBot, a self-diagnosis chatbot used by over
16,000 users in China, struggled with generalization and usage outside its training scope, despite high
test scores [14].
Recent work from UMass Amherst found hallucinations in â€œalmost allâ€ AI-generated medical
summaries by top LLMs including GPT-4o and LLaMA-3 [40]. These systems, while objectively
fluent, imposed hidden verification burdens on clinicians. Trust calibration remained low, and the
promised 40% workload reduction often went unrealized due to poor integration into existing routines.
4
When these issues surface post-deployment, systems are typically downgraded to limited advisory
roles. Studies estimate that projected ROI drops by 70â€“80% [12], revealing how failure to evaluate
human, temporal, and contextual dimensions undermines deployment success.
Here failures reflect neglect of human-centered, temporal, and contextual dimensions, despite high
technical scores.
4.2
Financial Services: Investment Agents Vulnerable to Market Shifts
In finance, agentic AI systems assist with portfolio optimization and compliance, often excelling in
historical backtesting and rule adherence. Benchmark performance ranges from 85â€“90% accuracy on
simulated tasks [19].
Yet these systems frequently degrade under real-world volatility. A study found that performance
deteriorated rapidly within months of deployment [1], due to poor generalization in dynamic en-
vironments. Vydyanathan [41] highlights how autonomous agents, when left unmonitored, make
misaligned portfolio adjustments that violate human expectations.
Moreover, simultaneous reactions by AI agents to market shifts can produce emergent â€œherd behavior,â€
exacerbating volatility instead of stabilizing it [27]. This dynamic risk remains invisible to static
```

</details>

---

### 10. Evaluation Metrics.

**Score: 8.0/10**

**Evaluation:**
Score: 8/10

Reasoning:

Clarity and Writing Quality (2/2):
The section is well-written, clear, and logically structured. The text effectively communicates complex ideas about AI evaluation metrics with a narrative flow that is both engaging and informative. Technical concepts are explained in an accessible manner.

Technical Depth (2/2):
The section demonstrates significant technical depth by providing concrete examples across multiple domains (legal, retail, financial), highlighting nuanced challenges in AI system evaluation. The analysis goes beyond surface-level observations to explore systemic issues in AI performance assessment.

Novelty and Originality (1.5/2):
The content presents a novel perspective on AI evaluation, emphasizing the need to move beyond traditional technical metrics. The insight about "multidimensional evaluation" across human, temporal, and contextual axes is particularly original and thought-provoking.

Methodology Rigor (1.5/2):
While not a traditional methodology section, the text provides rigorous analysis through multiple real-world case studies. The examples from Air Canada, McDonald's, and DPD are well-chosen to illustrate systemic evaluation gaps.

Evidence and Support (1/2):
The section is well-supported with citations and specific examples. However, some claims could benefit from more detailed empirical evidence or deeper quantitative analysis.

Strengths:
- Compelling narrative about AI evaluation limitations
- Diverse, concrete examples
- Clear articulation of the "evaluation gap"

Areas for Improvement:
- Could provide more quantitative data
- Some claims might benefit from additional supporting evidence

The section effectively argues for a more holistic approach to AI system evaluation, making it a strong contribution to understanding AI performance assessment.

<details>
<summary>ğŸ“„ View Section Content (3097 characters)</summary>

```
evaluation metrics.
Legal and regulatory risks are mounting as well. A Canadian tribunal held Air Canada liable when its
AI assistant gave incorrect fare guidance [45], establishing that firms are accountable for AI missteps.
The U.S. Consumer Financial Protection Bureau similarly reported that poor chatbot design led to
widespread customer harm, fees, and trust breakdowns [5].
These examples underscore the need for financial AI evaluation to go beyond accuracy and compliance,
integrating stress tests, scenario robustness, and human-agent interpretability metrics.
Here evaluation failed to account for temporal, human-centered, and contextual vulnerabilities in
volatile and regulated domains.
4.3
Retail: Customer Support Systems That Damaged Experience
Retail AI agents often succeed in early testing: reducing handling time by 70â€“80% and passing
compliance with over 95% accuracy [9]. However, real-world use reveals significant customer
experience degradation.
These systems struggle with edge cases and nuanced interactions [26]. A prominent example was
McDonaldâ€™s AI drive-thru system, which failed after a multi-year collaboration with IBM. Viral videos
showed repeated misunderstandings, including one where the AI added 260 Chicken McNuggets to
an order [8]. The system was ultimately shut down.
DPDâ€™s delivery chatbot was manipulated into swearing at a customer and composing a self-critical
poem [15]. In New York, the MyCity chatbot dispensed illegal business advice, such as permitting
employers to fire workers for reporting harassment [24].
These incidents damaged brand trust and led to project cancellations. Although internal projections
often promise high ROIâ€”such as $0.67 profit per dollar invested [21]â€”they rarely account for fallout
in Net Promoter Score (NPS), repeat contacts, or cart abandonment, which routinely worsen by
15â€“40% [28].
Despite high technical efficiency, failures in human-centered experience and contextual alignment led
to business losses.
4.4
The Evaluation Gap
Across all three domains, we observe a consistent pattern: benchmark performance drove optimistic
ROI projections that failed to materialize. Reports estimate that agentic AI systems could contribute
$4.4 trillion in productivity gains [31], but realized returns are often less than 25% of forecasts [28].
5
This disconnect arises from systematically omitting or underweighting evaluation of human interac-
tion, adaptability over time, and domain-specific constraints. The result is a persistent gap between
perceived and actual value, exposing organizations to reputational, legal, and financial risks.
Key Insight: Narrow evaluation of technical metrics alone provides a misleading picture of system
readiness. Multidimensional evaluation, across human, temporal, and contextual axes, is essential for
deployment-aligned assessment.
5
A Balanced Framework for Evaluating Agentic AI
The failures highlighted in Section 4 are not isolated accidents. They reflect a systemic overemphasis
on technical correctness at the expense of human, temporal, and contextual factors. To realign
```

</details>

---

### 11. Evaluation With Real-World Success, We Propose A Framework That Balances These Dimensions, Supports

**Score: 9.0/10**

**Evaluation:**
Score: 9/10

Reasoning:

1. **Clarity and Writing Quality** (2/2):
- Exceptionally well-written with clear, precise language
- Complex concepts are explained systematically and logically
- Professional academic writing style with structured arguments

2. **Technical Depth** (2/2):
- Provides a sophisticated multi-dimensional framework for AI system evaluation
- Introduces four critical evaluation axes (Technical, Human-centered, Temporal, Contextual)
- Demonstrates deep understanding of complex system interactions
- Offers nuanced insights into interdimensional dependencies

3. **Novelty and Originality** (2/2):
- Presents a novel, holistic approach to evaluating AI systems
- Goes beyond traditional single-metric evaluation methods
- Introduces innovative concept of interdimensional system effects
- Provides unique insights into real-world AI deployment challenges

4. **Methodology Rigor** (1.5/2):
- Clear methodological framework with well-defined evaluation dimensions
- Systematic exploration of interdimensional interactions
- Slightly loses a half point for not detailing exact measurement methodologies

5. **Evidence and Support** (1.5/2):
- Includes practical examples from healthcare, finance, and retail
- Demonstrates real-world application of framework
- Lacks extensive empirical data/experimental validation
- Loses half point for not providing comprehensive quantitative evidence

The section represents an outstanding contribution to AI system evaluation methodology, offering a comprehensive, nuanced approach to understanding complex technological deployments across different domains.

<details>
<summary>ğŸ“„ View Section Content (5827 characters)</summary>

```
evaluation with real-world success, we propose a framework that balances these dimensions, supports
domain adaptation, and maintains practical feasibility.
5.1
Core Axes of Evaluation
Our framework is organized around four primary evaluation axes, each representing a distinct aspect
of system behavior:
Technical (T): Measures discrete system performance on well-defined tasks. This includes traditional
metrics such as success rates (Pass@k), accuracy, latency, resource efficiency, and structural fidelity.
These metrics are necessary foundations but insufficient predictors of deployment success.
Human-centered (H): Captures how users experience, interpret, and adapt to the system. This
dimension includes trust calibration (the alignment between system confidence and user trust),
usability (cognitive load, ease of use), collaboration quality (hand-off effectiveness, interruption
management), and mental model accuracy. These factors directly influence adoption rates and realized
performance.
Temporal (R): Assesses stability and adaptability over time. This includes performance drift (how
accuracy changes with shifting conditions), adaptation rates (learning curves for both system and
users), knowledge retention (consistency across sessions), and value alignment stability (resistance to
goal corruption). Temporal metrics are essential for systems that evolve during use and face changing
conditions.
Contextual (C): Evaluates alignment with domain-specific constraints and objectives. This includes
regulatory compliance, risk exposure (financial, reputational, safety), economic impact (ROI, effi-
ciency gains), and workflow integration. These metrics reflect how well the system functions within
organizational and sectoral ecosystems.
While distinctly defined, these dimensions are not independent variables but rather form an intercon-
nected system, as we explore in the next section.
5.2
Dimensional Interdependence
A key insight from our case studies is that dimensions donâ€™t operate in isolation. Instead, they form a
complex, interdependent system where changes in one dimension inevitably affect others (Fig. 1).
Technical â†”Human-centered: Technical performance directly shapes user trust and experience,
while human feedback and usage patterns influence technical effectiveness. In healthcare deployments,
even systems with >95% diagnostic accuracy failed when clinicians lacked calibrated trust, leading
them to either over-rely on or dismiss system recommendations.
Technical â†”Temporal: Technical design choices determine long-term stability and adaptability,
while temporal patterns reveal technical strengths and weaknesses. Financial AI systems that excelled
in optimization tasks based on historical market data but lacked robust adaptation mechanisms showed
significant performance degradation during market volatility.
Technical â†”Contextual: Technical capabilities define whatâ€™s possible within domain constraints,
while contextual factors establish requirements and limitations for technical approaches. In retail,
technically efficient systems that failed to align with customer emotional expectations damaged brand
perception and reduced sales.
6
Technical
Temporal
Human 
Centered
Contextual
Level of trust in users, 
and calibration from 
user feedback
Robustness 
over time
Evolution in 
user dynamics
Ability to adapt to 
changes in regulation
and business
Domain specific 
constraints
Alignment in 
domain
workflow
Figure 1: Interdependencies across agentic AI evaluative metric dimensions.
Human-centered â†”Temporal: User trust and mental models evolve over time, while system
predictability and stability shape user expectations and behaviors. Trust calibration issues often
compound over time, with initial minor discrepancies evolving into significant usage problems.
Human-centered â†”Contextual: Human experiences are shaped by organizational context and
workflow fit, while user behavior determines how contextual value is realized. Systems that failed to
integrate smoothly into established workflows were rejected despite strong technical performance.
Temporal â†”Contextual: Temporal adaptation must align with changing regulatory and business
environments, while contextual factors determine what kinds of adaptation are valuable or permissible.
Systems that couldnâ€™t adapt to seasonal retail patterns or evolving healthcare guidelines quickly lost
their initial value.
These interdependencies may explain why single-dimension evaluation approaches often fail to
predict real-world outcomes. Systems optimized solely for technical metrics may create unforeseen
problems in other dimensions that only become apparent during deployment.
5.3
Framework Implementation Across Domains
To make our framework practical across different sectors, we propose an implementation approach
that balances standardization with domain specificity. At the center lies a minimal core evaluation
set, one foundational metric per axis, sufficient to provide an initial balanced assessment. From this
foundation, each domain extends the framework with specialized metrics that address sector-specific
needs while maintaining awareness of cross-dimensional effects.
Healthcare implementations tend to prioritize technical accuracy and human trust, financial services
extend further on regulatory compliance and temporal stability, while retail emphasizes human
satisfaction and contextual business metrics.
The implementation process should explicitly track interdimensional effects. For example, when
improving diagnostic accuracy in healthcare, teams should simultaneously monitor changes in trust
calibration, alert fatigue, and workflow integration to capture ripple effects across the system.
This structure supports both cross-domain comparability and contextual depth. It also enables scalable
```

</details>

---

### 12. Evaluation That Is Minimum Viable Testing At Early Stages, And Deep Analysis Before Full Deployment.

**Score: 9.0/10**

**Evaluation:**
Score: 9/10

Reasoning:

1. Clarity and Writing Quality (2/2):
- Exceptionally clear and well-structured writing
- Technical concepts are explained coherently
- Logical flow between subsections
- Demonstrates sophisticated academic writing style

2. Technical Depth (2/2):
- Introduces a novel metric interaction formalism
- Provides mathematical framework for multi-dimensional system evaluation
- Demonstrates nuanced understanding of AI system assessment
- Offers concrete, practical guidance for implementation

3. Novelty and Originality (2/2):
- Challenges conventional single-metric evaluation approaches
- Proposes comprehensive, multi-dimensional assessment framework
- Introduces innovative thinking about AI system effectiveness
- Bridges technical and human-centered perspectives

4. Methodology Rigor (2/2):
- Presents clear mathematical formulation for effectiveness scoring
- Anticipates and systematically addresses potential counterarguments
- Provides detailed recommendations for research and industry
- Shows methodological sophistication in evaluation design

5. Evidence and Support (1/2):
- References previous sections (Section 3 and 4)
- Mentions existing evaluation instruments
- Could benefit from more explicit empirical evidence
- Slightly deducted for limited direct experimental validation

The section represents high-quality academic writing with a sophisticated approach to AI system evaluation, offering both theoretical framework and practical implementation guidance. Its comprehensive treatment of multi-dimensional assessment makes it exceptionally strong.

<details>
<summary>ğŸ“„ View Section Content (4602 characters)</summary>

```
evaluation that is minimum viable testing at early stages, and deep analysis before full deployment.
5.4
Metric Interaction Formalism
To formalize our approach to balancing multiple metrics, we define a systemâ€™s real-world effectiveness
score. Let T, H, R, and C be normalized scores (0â€“1) for technical, human-centered, temporal, and
contextual metrics respectively. We define a systemâ€™s real-world effectiveness score U as:
7
U = wT T + wHH + wRR + wCC
where
X
wi = 1
In current practice, wT â‰ˆ1 and all others are near zero, implicitly treating technical success as a
proxy for overall value. But evidence from Section 3 and Section 4 shows this fails to predict actual
outcomes.
We argue that deployment-critical use cases (e.g., clinical decision support, financial compliance)
require a balanced weighting scheme. For instance, (wT , wH, wR, wC) = (0.3, 0.25, 0.2, 0.25),
with calibration dependent on risk tolerance and domain complexity.
This formalism clarifies trade-offs and helps guide decisions during evaluation design. A system
scoring 0.9 on T but 0.2 on H and R may appear strong under conventional evaluation, yet would
score only 0.55 under our balanced framework, correctly flagging potential deployment issues.
Beyond simple weighted combinations, advanced implementations should also consider interaction
effects between dimensions. For example, the combination of low trust calibration H with high
adaptation rate R can create particularly problematic outcomes in financial systems responding to
market volatility.
Organizations can implement this framework through a phased approach that balances thoroughness
with practical constraints.Appendix A.2 provides a practical implementation example.
5.5
Anticipating and Addressing Counterarguments
We anticipate four common objections to expanding evaluation beyond technical metrics:
â€œHuman-centered metrics are too subjective to trust.â€ Response: Subjectivity is not noise. It
reflects user experience, which governs adoption and safety. Instruments like TrAAIT, NASA-TLX,
and trust calibration curves have shown strong correlation with deployment outcomes. These can be
standardized, validated, and benchmarked.
â€œSafety and governance are outside the research scope; leave them to regulators.â€ Response:
Safety, like accuracy, is an engineering problem first. Early design decisions shape emergent behaviors
and risk profiles. Deferring evaluation until regulation is reactive and dangerous especially for rapidly
evolving systems.
â€œMore metrics slow development and hinder innovation.â€ Response: Our framework is modular
and scalable. Core metrics can be collected with minimal overhead, while deeper evaluation can
be phased in during later stages. In fact, catching trust or adaptation issues early often accelerates
deployment by reducing backtracking.
â€œOne-size-fits-all frameworks canâ€™t handle domain differences.â€ Response: Thatâ€™s why our
quadrant is extensible. The inner core supports cross-domain baselining, while outer layers incor-
porate sector-specific requirements. This mirrors how safety standards operate in aerospace vs.
pharmaceuticals; different in detail, unified in structure.
This framework is not just a theoretical construct, it is designed for implementation. The next section
provides concrete recommendations for researchers, industry practitioners, and policymakers to adopt,
extend, and apply the model in practice.
6
Recommendations and Research Agenda
Addressing the measurement imbalance in agentic AI requires coordinated action across research,
deployment, and governance communities. We propose the following priorities:
6.1
Research Community
Develop and validate human-centered evaluation instruments and temporal metrics across domains.
Design cross-domain benchmarks that integrate human dimensions, safety, and long-term perfor-
mance beyond task correctness.
8
Create lightweight evaluation toolkits based on our quadrant framework, while incentivizing
multidimensional reporting in publications through structured requirements.
Bridge disciplines by fostering collaboration between technical AI fields and HCI, safety engineering,
and organizational science.
6.2
Industry Practitioners
Implement comprehensive pre-deployment evaluations across all four axes (technical, human,
temporal, contextual) and track longitudinal agent behavior.
Adopt trust-focused approaches by measuring user interpretation and calibration early, while
integrating staged evaluation into existing workflows.
Enhance evaluation quality by including domain experts in metric design and transparently reporting
```

</details>

---

### 13. Limitations.

**Score: 5.0/10**

**Evaluation:**
Score: 5/10

Reasoning:

Clarity and Writing Quality (1/2):
- The text is somewhat fragmented and lacks clear, complete sentences
- The language is technical and dense, making it challenging to immediately understand
- Appears more like bullet points or notes than a fully developed section

Technical Depth (1/2):
- Contains potentially interesting policy recommendations
- Introduces concepts like quadrant model and multidimensional reporting
- However, lacks specific elaboration or concrete implementation details
- Recommendations feel somewhat abstract and generalized

Novelty and Originality (1/2):
- Suggests some innovative approaches to AI policy and evaluation
- Mentions interesting concepts like "safe harbors" for testing evaluation methods
- But lacks substantive explanation of how these novel ideas would be practically implemented

Methodology Rigor (1/2):
- No clear methodology is presented
- Recommendations are high-level policy suggestions without substantive backing
- No explanation of how these recommendations were derived

Evidence and Support (1/2):
- No citations or empirical evidence supporting the recommendations
- No reference to existing frameworks or prior work
- Assertions are made without substantive justification

The section reads more like preliminary policy notes than a robust limitations discussion. While it hints at interesting ideas, it requires significant expansion, clarity, and evidential support to be considered a strong research limitations section.

<details>
<summary>ğŸ“„ View Section Content (474 characters)</summary>

```
limitations.
6.3
Policymakers
Mandate and standardize human-centered metrics and longitudinal tracking for high-stakes AI
applications.
Fund open-source evaluation suites aligned with the quadrant model, while coordinating cross-
sector guidelines that balance harmonization with local context.
Ensure accountability through multidimensional reporting requirements for public-sector deploy-
ments, while creating regulatory â€œsafe harborsâ€ for testing new evaluation methods.
```

</details>

---

### 14. Evaluation Forms The Foundation For Trust, Effectiveness, And Accountability In Ai Systems. This

**Score: 3.0/10**

**Evaluation:**
Score: 3/10

Reasoning:
This section has significant limitations across the evaluation criteria:

1. Clarity and Writing Quality (1/2):
- The text is very short and lacks coherent structure
- The sentence is grammatically incomplete
- The sudden inclusion of "7" at the end is confusing and appears to be an errant notation

2. Technical Depth (1/2):
- While it mentions important concepts like trust, effectiveness, and accountability in AI systems, it provides no substantive explanation
- No specific details about evaluation methods or frameworks are presented
- The claims are vague and unsupported

3. Novelty and Originality (0/2):
- No novel insights are presented
- The statement reads like a generic platitude about AI evaluation
- No unique perspectives or innovative approaches are discussed

4. Methodology Rigor (0/2):
- No methodological details are provided
- No explanation of how evaluation actually contributes to trust or accountability
- No research approach or framework is outlined

5. Evidence and Support (1/2):
- No citations or evidence support the claims
- No specific examples or research references are included

The section appears to be an incomplete or fragmentary draft that requires substantial development to be considered a meaningful research contribution. It would need significant expansion, clarity, and substantive content to be considered a quality research section.

<details>
<summary>ğŸ“„ View Section Content (194 characters)</summary>

```
Evaluation forms the foundation for trust, effectiveness, and accountability in AI systems. This
agenda offers a path toward more comprehensive and deployment-aligned assessment of agentic AI.
7
```

</details>

---

### 15. Conclusion And Call To Action

**Score: 7.0/10**

**Evaluation:**
Score: 7/10

Reasoning:

Clarity and Writing Quality (1.5/2):
The text is concise and well-structured, with a clear articulation of the paper's key argument. The language is professional and academic, effectively summarizing the core message of the research.

Technical Depth (1.5/2):
The section demonstrates technical sophistication by highlighting the limitations of current AI evaluation frameworks. The mention of a meta-analysis of 84 publications and cross-domain case studies suggests a robust analytical approach.

Novelty and Originality (1/2):
The critique of current AI evaluation metrics appears innovative, pointing out the gap between technical benchmarks and real-world deployment success. However, the excerpt is cut off, which limits a full assessment of the novelty.

Methodology Rigor (1/2):
There's a hint of methodological rigor with the reference to a meta-analysis and cross-domain case studies. However, without seeing the full details, the score is slightly conservative.

Evidence and Support (1/2):
The section suggests evidence-based findings, mentioning a meta-analysis and case studies. But the brevity of the excerpt means the full extent of supporting evidence isn't completely clear.

Strengths:
- Clear, focused argument
- Highlights important limitations in current AI evaluation
- Suggests a comprehensive research approach

Areas for Potential Improvement:
- Complete the call to action
- Provide more specific insights from the meta-analysis
- Elaborate on the proposed unifying framework

The score reflects a solid, promising research conclusion with room for more detailed exposition.

<details>
<summary>ğŸ“„ View Section Content (507 characters)</summary>

```
Conclusion and Call to Action
This paper has argued that current evaluation frameworks for agentic AI systems are dangerously
imbalanced, privileging technical metrics while neglecting the human, temporal, and contextual
dimensions that determine real-world value. Through a meta-analysis of 84 publications, cross-
domain case studies, and a unifying framework, we have shown that benchmark success does not
guarantee deployment success. As agents move from labs to high-stakes settings, our instruments of
```

</details>

---

### 16. Evaluation Must Evolve.

**Score: 7.0/10**

**Evaluation:**
Score: 7/10

Reasoning:

Clarity and Writing Quality (1.5/2):
The text is concise and clear, presenting a compelling argument for the evolution of AI system evaluation. The bullet points are well-structured and thought-provoking, making the key points easy to understand.

Technical Depth (1.5/2):
The section demonstrates technical sophistication by highlighting critical dimensions of AI evaluation beyond traditional benchmarking. The questions posed show nuanced thinking about system assessment, particularly emphasizing safety, fairness, and contextual adaptability.

Novelty and Originality (1.5/2):
The content offers an innovative perspective on AI evaluation, moving beyond narrow performance metrics to consider broader systemic implications. The focus on longitudinal testing, ecosystem integration, and sustainable performance is particularly novel.

Methodology Rigor (1/2):
While the section raises excellent questions, it doesn't provide a detailed methodology for implementation. It's more of a provocative call to action than a rigorous research proposal.

Evidence and Support (1.5/2):
The section lacks direct citations or empirical evidence, but it effectively frames critical research challenges. The language suggests a well-informed perspective on the limitations of current evaluation approaches.

Strengths:
- Compelling vision for next-generation AI evaluation
- Addresses crucial interdisciplinary considerations
- Invites collaborative problem-solving

Areas for Improvement:
- Could benefit from more specific research directions
- Needs supporting evidence or preliminary frameworks
- Lacks concrete implementation suggestions

The score reflects a strong, thought-provoking piece that sets an important agenda for AI evaluation research.

<details>
<summary>ğŸ“„ View Section Content (690 characters)</summary>

```
evaluation must evolve.
Looking ahead, the next generation of evaluation must address several urgent questions:
â€¢ How can we standardize trust, collaboration, and workflow-fit metrics across domains
without sacrificing contextual nuance?
â€¢ What are the minimal viable longitudinal tests that predict system degradation, adaptation,
or failure modes before full-scale deployment?
â€¢ Can we design benchmarks that reflect not only agent intelligence but also their ability to
operate safely, fairly, and sustainably in complex ecosystems?
We invite the community to help build the next era of agentic AI evaluationâ€”one that reflects how
systems succeed in the world, not just on the benchmark.
```

</details>

---

### 17. References

**Score: 7.0/10**

**Evaluation:**
Score: 7/10

Reasoning:
The references section demonstrates several positive qualities:

Strengths:
- Recent and current sources (2018-2025)
- Diverse sources from different academic venues (conferences, journals, review publications)
- Covers relevant topics in AI, technology, and emerging research domains
- Includes conference proceedings, academic journals, and strategic reviews

Areas for Improvement:
- Some references appear incomplete (cut off mid-citation in [4])
- Slight inconsistency in citation formatting
- Could benefit from more diverse sources (currently seems tech/AI-heavy)

Breaking down the scoring criteria:
1. Clarity and Writing Quality: 2/2 - Clear, standard reference formatting
2. Technical Depth: 1.5/2 - Covers technically sophisticated topics
3. Novelty and Originality: 1/2 - References seem current but not groundbreaking
4. Methodology Rigor: 1.5/2 - References suggest rigorous research approaches
5. Evidence and Support: 1/2 - Good range of sources, but incomplete final reference

The references show promise and relevance to contemporary AI research, with room for minor refinements in completeness and diversity. The section effectively supports the likely technological focus of the research paper.

<details>
<summary>ğŸ“„ View Section Content (821 characters)</summary>

```
References
[1] N. Abbas, C. Cohen, D. J. Grolleman, and B. Mosk. Artificial intelligence can make markets more
efficientâ€”and more volatile. Strategic HR Review, October 2018.
[2] S. Agashe, J. Han, S. Gan, J. Yang, A. Li, and X. E. Wang. Agent S: an open agentic framework that uses
computers like a human. 2024 IEEE International Conference on Big Data (BigData), 2024.
9
[3] B. Arslan, C. Nuhoglu, M. Satici, and E. Altinbilek. Evaluating llm-based generative AI tools in emergency
triage: A comparative study of chatgpt plus, copilot pro, and triage nurses. The American Journal of
Emergency Medicine, 2025.
[4] B. Bogin, K. Yang, S. Gupta, K. Richardson, E. Bransom, P. Clark, A. Sabharwal, and T. Khot. Super:
evaluating agents on setting up and executing tasks from research repositories. In Conference on Empirical
```

</details>

---

### 18. Methods In Natural Language Processing, 2024.

**Score: 6.0/10**

**Evaluation:**
Score: 6/10

Reasoning:

Clarity and Writing Quality (1/2):
- The section appears to be a reference list rather than a cohesive methodological description
- Citations are correctly formatted but lack a clear narrative structure
- Lacks clear explanation of NLP methods

Technical Depth (1/2):
- Contains some interesting references to AI and NLP applications
- References span multiple domains (healthcare, finance, software development)
- Lacks substantive technical details about NLP methodologies
- Mix of academic and news sources reduces technical credibility

Novelty and Originality (1/2):
- Some novel references to emerging AI agent research (e.g., GitHub Copilot, MLAgentBench)
- Covers contemporary AI applications across different sectors
- Limited depth in explaining innovative approaches

Methodology Rigor (1/2):
- No clear methodological framework presented
- References suggest potential research directions but don't provide rigorous methodology
- Lacks systematic approach to NLP method evaluation

Evidence and Support (2/2):
- Diverse range of citations from academic journals, conferences, and reports
- References from recent years (2020-2025)
- Good breadth of sources across different domains

The section needs significant improvement in presenting a coherent methodology for Natural Language Processing. While the references are current and diverse, they don't constitute a robust methodological description.

<details>
<summary>ğŸ“„ View Section Content (2944 characters)</summary>

```
Methods in Natural Language Processing, 2024.
[5] C. F. P. Bureau. Chatbots in consumer finance. Technical report, Consumer Financial Protection Bureau,
2025.
[6] B. Cao, S. Huang, and W. Tang. AI triage or manual triage? exploring medical staffsâ€™ preference for AI
triage in china. Patient Education and Counseling, page 108076, 2024.
[7] H. Clatterbuck, C. Castro, and A. M. MorÃ¡n. Risk alignment in agentic AI systems, October 2024.
arXiv:2410.01927 [cs].
[8] J. Creswell. 260 McNuggets? McDonaldâ€™s Ends A.I. Drive-Through Tests Amid Errors. New York Times,
June 2020. Accessed: 2025-05-15.
[9] I. M. De Andrade and C. Tumelero. Increasing customer service efficiency through artificial intelligence
chatbot. Revista de GestÃ£o, 29(3):238â€“251, 2022.
[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image
database. 2009.
[11] Y. Deng, L. Liao, Z. Zheng, G. H. Yang, and T.-S. Chua. Towards human-centered proactive conversational
agents. In ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR â€™24,
New York, NY, USA, July 2024. Association for Computing Machinery.
[12] N. Eddy. Health systems chase ROI, target efficiency in AI for 2025. Health Informatics, February 2016.
[13] eMarketer. 2024: The year AI agents transformed from chatbots to productivity powerhouses, January
2025.
[14] X. Fan, D. Chao, Z. Zhang, D. Wang, X. Li, and F. Tian. Utilization of self-diagnosis health chatbots in
real-world settings: case study. Journal of Medical Internet Research, 2021.
[15] T. Gerken. Dpd error caused chatbot to swear at customer. International Journal of Scientific Research in
Engineering and Management, January 2024.
[16] GitHub. Github copilot workspace, 2024. Information about GitHub Copilot Workspace, an AI-powered
development environment for software debugging and coding assistance.
[17] W. D. Heaven. Hundreds of AI tools have been built to catch covid. none of them helped. MIT Technology
Review, July 2021.
[18] Q. Huang, J. Vora, P. Liang, and J. Leskovec. Mlagentbench: evaluating language agents on machine
learning experimentation. In International Conference on Machine Learning, pages 20271â€“20309. PMLR,
2023.
[19] L. Hughes, Y. K. Dwivedi, T. Malik, M. Shawosh, M. A. Albashrawi, I. Jeon, V. Dutot, M. Appanderanda,
T. Crick, R. Deâ€™, et al. AI agents and agentic systems: A multi-expert analysis. Journal of Computer
Information Systems, 2025.
[20] S. K. Jeyakumar, A. A. Ahmad, and A. G. Gabriel. Advancing agentic systems: dynamic task decom-
position, tool integration and evaluation using novel metrics and dataset. In NeurIPS 2024 Workshop on
Open-World Agents, 2024.
[21] A. Karuparti, A. Umachandran, T. Webb, B. Czernicki, S. Lacasse, and V. Pamula. A framework for
calculating ROI for agentic AI apps, February 2025.
[22] M. Khalifa and M. Albadawy. AI in diagnostic imaging: revolutionising accuracy and efficiency. Computer
```

</details>

---

### 19. Methods And Programs In Biomedicine Update, 2024.

**Score: 8.0/10**

**Evaluation:**
Score: 8/10

Reasoning:

Strengths (contributing to high score):
1. Clarity and Writing Quality (2/2):
- Exceptionally clear and structured methodology description
- Well-defined inclusion criteria for the meta-analysis
- Precise articulation of coding schema and evaluation dimensions

2. Technical Depth (2/2):
- Sophisticated multi-dimensional coding approach
- Explicit definition of each evaluation dimension (Technical Performance, Human-Centered, Safety/Governance, Economic Impact)
- Provides detailed criteria for paper inclusion and coding

3. Methodology Rigor (2/2):
- Clearly defined timeframe (January 2023 to April 2025)
- Specific system type requirements
- Binary vector coding method demonstrates systematic approach
- Transparent evaluation evidence requirements

4. Novelty and Originality (1/2):
- Interesting framework for categorizing agentic AI literature
- Comprehensive meta-analysis approach
- Slight reduction in novelty score as meta-analysis methodology is relatively established

5. Evidence and Support (1/2):
- References to multiple publications
- Concrete coding criteria
- Could benefit from more explicit examples of how coding is applied

The section provides a robust, well-structured methodology for analyzing agentic AI literature across multiple dimensions, demonstrating high-quality research design with clear, systematic evaluation criteria.

<details>
<summary>ğŸ“„ View Section Content (6195 characters)</summary>

```
Methods and Programs in Biomedicine Update, 2024.
[23] J. Y. Koh, R. Lo, L. Jang, V. Duvvur, M. Lim, P.-Y. Huang, G. Neubig, S. Zhou, R. Salakhutdinov, and
D. Fried. VisualWebArena: evaluating multimodal agents on realistic visual web tasks. In Proceedings of
the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024.
10
[24] C. Lecher. NYC AI chatbot touted by adams tells businesses to break the law. International Journal of
Scientific Research and Engineering Trends, March 2025.
[25] T. Lee, M. Yasunaga, C. Meng, Y. Mai, J. S. Park, A. Gupta, Y. Zhang, D. Narayanan, H. Teufel,
M. Bellagente, et al. Holistic evaluation of text-to-image models. Conference on Empirical Methods in
Natural Language Processing, 36, 2024.
[26] Manhattan Associates. Reimagine customer service with agentic AI for retail. Journal of Services
Marketing, May 2022.
[27] D. Marla. How agentic AI will change financial services, February 2023.
[28] C. Michael, E. Hazan, and R. Roberts. The economic potential of generative AI: the next productivity
frontier. Technical report, McKinsey & Company, 2024.
[29] C. Mitelut, B. Smith, and P. Vamplew. Intent-aligned AI systems deplete human agency: the need for
agency foundations research in AI safety, May 2023. arXiv:2305.19223 [cs].
[30] D. Moshkovich, H. Mulian, S. Zeltyn, N. Eder, I. Skarbovsky, and R. Abitbol. Beyond black-box bench-
marking: observability, analytics, and optimization of agentic systems. Annual Conference Companion on
Genetic and Evolutionary Computation Conference: Late Breaking Papers, 2009.
[31] C. Mueller, D. Piasecki, M. E. Hoyek, and O. Cheta. How COOs maximize operational impact from gen
AI and agentic AI. Spectrum of Emerging Sciences, 2024.
[32] D. Roman, N. Ari, and J. Mell. The harmony index: evaluating, predicting, and visualizing effectiveness
in multi-agent team dynamics. In AAAI Conference on Artificial Intelligence and Interactive Digital
Entertainment, 2024.
[33] S. Rome, T. Chen, R. Tang, L. Zhou, and F. Ture. " ask me anything": how comcast uses llms to assist
agents in real time. In ACM SIGIR Conference on Research and Development in Information Retrieval,
2024.
[34] D. Rostcheck and L. Scheibling. The elephant in the room: why AI safety demands diverse teams. In
Future of Information and Communication Conference, pages 357â€“369. Springer, 2025.
[35] D. Sheeran and T. Kass-Hout. How agentic AI systems can solve the three most pressing problems in
healthcare today, 2024.
[36] A. F. Stevens and P. Stetson. Theory of trust and acceptance of artificial intelligence technology (TrAAIT).
Journal of Biomedical Informatics, 2023.
[37] S. S. Sundar. Rise of machine agency: A framework for studying the psychology of humanâ€“AI interaction
(HAII). Journal of Computer-Mediated Communication, 2020.
[38] X. Tang, Y. Liu, Z. Cai, Y. Shao, J. Lu, Y. Zhang, Z. Deng, H. Hu, K. An, R. Huang, S. Si, S. Chen,
H. Zhao, L. Chen, Y. Wang, T. Liu, Z. Jiang, B. Chang, Y. Fang, Y. Qin, W. Zhou, Y. Zhao, A. Cohan,
and M. Gerstein. Ml-bench: evaluating large language models and agents for machine learning tasks on
repository-level code. International Conference on Learning Representations, 2025.
[39] K. Valmeekam, M. Marquez, A. Olmo, S. Sreedharan, and S. Kambhampati. Planbench: an extensible
benchmark for evaluating large language models on planning and reasoning about change. Advances in
Neural Information Processing Systems, 2025.
[40] P. R. Vishwanath, S. Tiwari, T. G. Naik, S. Gupta, D. N. Thai, W. Zhao, S. KWON, V. Ardulov, K. Tarabishy,
A. McCallum, et al. Faithfulness hallucination detection in healthcare AI. In Artificial Intelligence and
Data Science for Healthcare: Bridging Data-Centric AI and People-Centric Healthcare, 2025.
[41] N. Vydyanathan. Smart investing with agentic AI: outsourcing the financial thinking. World Journal of
Advanced Engineering Technology and Sciences, 2023.
[42] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A multi-task benchmark
and analysis platform for natural language understanding. In International Conference on Learning
Representations, 2018.
[43] B. Xia, Q. Lu, L. Zhu, and Z. Xing. An AI system evaluation framework for advancing AI safety:
Terminology, taxonomy, lifecycle mapping. In ACM International Conference on AI-Powered Software,
2024.
11
[44] Y. Xiao, E. Sun, D. Luo, and W. Wang. Tradingagents: multi-agents LLM financial trading framework.
Ovidius University Annals. Economic Sciences Series, 2025.
[45] M. Yagoda. Airline held liable for its chatbot giving passenger bad advice â€“ what this means for travellers.
The British Broadcasting Corporation, 2024.
[46] Y. Yu, H. Li, Z. Chen, Y. Jiang, Y. Li, D. Zhang, R. Liu, J. W. Suchow, and K. Khashanah. Finmem: A
performance-enhanced llm trading agent with layered memory and character design. In AAAI Symposium
Series, 2024.
A
Appendices
A.1
Coding Schema for Meta-Analysis
To characterize the evaluation focus of recent agentic AI literature across four key metric dimensions:
technical performance, human-centered evaluation, safety/governance, and economic impact. Each
publication was coded based on whether it included at least one metric or instrument aligned with
each dimension. Both peer-reviewed academic publications and industry white papers were eligible.
Papers were included in the review if they met all of the following:
â€¢ Timeframe: Published between January 2023 and April 2025
â€¢ System Type: Described evaluation of an agentic AI system (LLM-based agents, multi-agent
systems, autonomous decision-makers, or LLM tool users)
â€¢ Evaluation Evidence: Included at least one stated metric or evaluation result related to
agent/system performance, user interaction, safety, or deployment outcomes
Each paper was coded using a binary vector [T, H, S, E], corresponding to Table 1. Papers were
Code
Dimension
Criteria for Inclusion
T
Technical Performance
Includes any of the following: task success (e.g., Pass@k, accuracy),
latency, resource usage (token count, memory), structural alignment
(e.g., Tool F1, Node F1), robustness to noise or adversarial
examples in narrow performance settings
H
Human-Centered
```

</details>

---

### 20. Evaluation

**Score: 6.0/10**

**Evaluation:**
Score: 6/10

Reasoning:

Clarity and Writing Quality (1.5/2):
- The section is concise and uses clear, structured language
- Organized into three key evaluation dimensions
- Good use of bullet points and short descriptive phrases
- Slight deduction for very abbreviated style

Technical Depth (1/2):
- Provides a high-level overview of evaluation dimensions
- Lists potential metrics and evaluation approaches
- Lacks specific details about how these metrics would be implemented
- Seems more like a framework or checklist than a deep technical evaluation methodology

Novelty and Originality (1/2):
- Covers standard evaluation dimensions for AI/technology assessment
- Includes some interesting modern considerations (e.g., value alignment, code-based autonomy)
- Not particularly groundbreaking, but comprehensive

Methodology Rigor (1/2):
- Outlines potential evaluation areas
- No detailed methodology for actually conducting these evaluations
- Provides a conceptual framework rather than a rigorous research approach

Evidence and Support (1.5/2):
- Includes specific references to established frameworks (NASA-TLX, NPS)
- Demonstrates awareness of multiple evaluation dimensions
- No direct evidence or experimental results presented

The section serves as a solid high-level evaluation framework, but would benefit from more detailed explanation and specific implementation guidance. It provides a useful overview but lacks depth for a comprehensive research evaluation.

<details>
<summary>ğŸ“„ View Section Content (733 characters)</summary>

```
Evaluation
Includes user trust (surveys, retention, calibration), usability or
workload measures (e.g., NASA-TLX), collaboration effectiveness,
workflow integration, simulated human ratings (e.g.,
Agent-as-a-Judge), satisfaction/NPS, human-in-the-loop A/B tests
S
Safety & Governance
Includes adversarial robustness, value alignment, error recovery,
failure rate analysis, regulatory compliance metrics, explainability
metrics, fairness audits, or code-based autonomy scoring (e.g.,
orchestration code inspection)
E
Economic & Business
Impact
Includes cost savings, ROI, time savings, process efficiency,
conversion rate improvement, customer lifetime value, productivity
uplift, or KPIs tied to organizational or business outcomes
Q
```

</details>

---

### 21. Acknowledgment, Multiple Evaluation Methods, Reproducibility

**Score: 9.0/10**

**Evaluation:**
Score: 9/10

Reasoning:

1. Clarity and Writing Quality (2/2):
- The section is exceptionally well-written and structured
- Clear, precise language with technical sophistication
- Logically organized with multiple subsections providing comprehensive details
- Professional academic writing style

2. Technical Depth (2/2):
- Extremely detailed technical framework for evaluating AI agent systems
- Provides nuanced multi-dimensional evaluation metrics (Technical, Human-Centered, Safety, Economic)
- Offers granular checklists and structured methodological approach
- Demonstrates deep understanding of complex AI evaluation challenges

3. Novelty and Originality (2/2):
- Introduces innovative framework for holistic AI system evaluation
- Addresses critical "measurement imbalance" in current AI assessment approaches
- Proposes novel phased implementation strategy
- Provides comprehensive taxonomy of evaluation dimensions

4. Methodology Rigor (2/2):
- Systematic evaluation methodology with clear inclusion criteria
- Structured checklist with specific, measurable questions
- Transparent process for resolving ambiguities
- Includes multiple validation approaches

5. Evidence and Support (1/2):
- Includes visual representations (figures)
- Provides detailed definitions and metrics
- Slightly limited in direct empirical evidence/citations
- Minor deduction for not extensively referencing external research

The section represents an exceptional contribution to AI system evaluation methodology, offering a sophisticated, multi-dimensional framework that addresses critical assessment challenges. Its comprehensive approach and clear articulation make it highly valuable for researchers and practitioners.

<details>
<summary>ğŸ“„ View Section Content (6705 characters)</summary>

```
acknowledgment, multiple evaluation methods, reproducibility
information, or statistical validation of results
Table 1: Coding Dimensions and Definitions.
independently reviewed twice with a structured evaluation checklist (see below). Any ambiguity were
resolved by a third reviewer. Full data and coded records are available in the accompanying CSV file.
Metric Inclusion Checklist
For each publication, the following yes/no questions were used:
â€¢ T1: Does the paper report a performance metric on a specific task or benchmark?
â€¢ T2: Is any computational or latency efficiency reported?
â€¢ T3: Does the paper compare performance across different model versions, sizes, or against
baseline systems?
12
Technical
Human-Centered
Temporal
Contextual
Phase 1
Phase 2
Phase 3
Phase 4
Figure 2: Framework implementation through a phased approach.
â€¢ H1: Are human users (or proxies) involved in the evaluation?
â€¢ H2: Are trust, satisfaction, collaboration, or user behavior metrics reported?
â€¢ H3: Does the paper measure learnability, ease of use, or cognitive load aspects of the agent?
â€¢ S1: Are safety, failure rates, or edge case behaviors explicitly measured?
â€¢ S2: Are governance, compliance, or alignment frameworks included?
â€¢ S3: Are bias, fairness, ethical considerations, or representational harms measured or dis-
cussed with metrics?
â€¢ E1: Is business impact, ROI, or operational efficiency tracked?
â€¢ E2: Are KPIs from real-world deployment or simulations discussed?
â€¢ E3: Is scalability, cost-effectiveness at scale, or long-term economic viability evaluated?
â€¢ Q1: Is the evaluation methodology described in sufficient detail for reproducibility?
â€¢ Q2: Does the paper explicitly acknowledge limitations of the evaluation approach?
â€¢ Q3: Does the evaluation use multiple, complementary methods (triangulation) to validate
findings?
If any question within a category was answered â€œyesâ€ that dimension received a 1. If metrics simulate
human experience (e.g., using an LLM to approximate user satisfaction), the paper was still marked
H=1, with a flag for â€œsimulated humanâ€ noted separately. Studies reporting only unstructured user
impressions were excluded unless accompanied by coded instruments, clear KPIs, or quantifiable
outcomes. Papers focused on emergent behavior or coordination were marked T=1, and S=1 if failure
modes or robustness were analyzed.
A.2
Practical Implementation Process
Organizations can implement this framework through a phased approach that balances thoroughness
with practical constraints:
â€¢ Baseline Assessment (phase 1): Establish core metrics across all four dimensions.
â€“ Technical: Benchmark task completion and accuracy
â€“ Human: Conduct initial trust calibration studies
13
â€“ Temporal: Measure baseline stability in controlled conditions
â€“ Contextual: Assess initial workflow fit and compliance requirements
â€¢ Domain Adaptation (phase 2): Select and calibrate domain-specific extensions
â€“ Identify sector-specific validation instruments
â€“ Define interdependence monitoring approach
â€“ Establish evaluation thresholds based on deployment criticality
â€¢ Pilot Evaluation (phase 3): Apply the framework to controlled deployments
â€“ Collect longitudinal data across all dimensions
â€“ Monitor cross-dimensional effects
â€“ Adjust weights based on observed interdependencies
â€¢ Full Integration (ongoing): Incorporate into development cycles and deployment decisions
â€“ Maintain dimensional balance throughout scaling
â€“ Regular reassessment as system and context evolve
â€“ Proactive monitoring of emerging interdependence effects
This phased approach balances comprehensiveness with real-world constraints, allowing organizations
to begin shifting toward balanced evaluation without disrupting innovation cycles (Fig. 2).
A.3
Metric Type Distribution
Fig. 3 displays three measures for each metric type: absolute number of papers, percentage of all
examined papers, and percentage of qualified papers that met quality thresholds. Note that individual
papers often employed multiple evaluation approaches.
Metric Type
Technical 
Human-Centered
Safety & Governance
Economic Impact
0
22.5
45
67.5
90
30%
53%
30%
83%
18%
33%
18%
51%
25
45
25
70
# of papers
% of all papers
% of qualified papers
Figure 3: Distribution of evaluation metric types across agentic AI papers (N = 138).
A.4
Terms and Definitions
Agentic AI systems: Systems characterized by: (1) goal-directed behavior with the ability to decom-
pose complex objectives into manageable subtasks; (2) environmental awareness and adaptability to
changing conditions; (3) tool utilization, where agents strategically leverage external resources to
accomplish tasks; and (4) autonomous decision-making with limited human intervention.
Measurement imbalance: The systemic bias in evaluation frameworks that privilege easily quan-
tifiable technical metrics while neglecting dimensions critical to real-world deployment success;
14
especially human-centered factors, temporal stability, and contextual fit. This creates a fundamental
misalignment between what we measure and what determines system value.
Technical metrics: Measures discrete system performance on well-defined tasks.
â€¢ Task success (e.g., Pass@k, accuracy)
â€¢ Latency, resource usage (token count, memory)
â€¢ Structural alignment (e.g., Tool F1, Node F1)
â€¢ Robustness to noise or adversarial examples in narrow performance settings
â€¢ Computational or latency efficiency
â€¢ Performance comparisons across different model versions, sizes, or against baseline systems
Human-centered metrics: Captures how users experience, interpret, and adapt to the system.
â€¢ User trust (surveys, retention, calibration)
â€¢ Usability or workload measures (e.g., NASA-TLX)
â€¢ Collaboration effectiveness, workflow integration
â€¢ simulated human ratings (e.g., Agent-as-a-Judge)
â€¢ Satisfaction/NPS, human-in-the-loop A/B tests
â€¢ Learnability, ease of use, or cognitive load aspects
Safety & governance metrics: Evaluates alignment with safety requirements and governance
standards.
â€¢ Adversarial robustness, value alignment, error recovery
â€¢ Failure rate analysis, regulatory compliance metrics
â€¢ Explainability metrics, fairness audits
â€¢ Code-based autonomy scoring (e.g., orchestration code inspection)
â€¢ Bias, fairness, ethical considerations, or representational harms measurements
Economic & business impact metrics: Assesses financial and operational value creation.
â€¢ Cost savings, ROI, time savings, process efficiency
â€¢ Conversion rate improvement, customer lifetime value
â€¢ Productivity uplift, or KPIs tied to organizational or business outcomes
â€¢ KPIs from real-world deployment or simulations
â€¢ Scalability, cost-effectiveness at scale, or long-term economic viability
15
```

</details>

---

## Technical Details

- **PDF Path**: `papers/Top1-2506.02064v1.pdf`
- **Processing Configuration**:
  - max_chunk_size: 4000
  - min_chunk_size: 100
  - delay_between_requests: 1.0

*Analysis generated by Research Paper Scorer v0.1.0*
